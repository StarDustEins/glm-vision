from __future__ import annotations

import os
import time
from datetime import datetime
from typing import Dict, List

import streamlit as st
from PIL import Image

from app_logic import (
    AnalysisResult,
    DEFAULT_MEDICAL_PROMPT,
    SUPPORTED_FORMATS,
    build_available_models,
    clear_analysis_cache,
    fetch_all_cache_records,
    fetch_cache_metrics,
    get_analysis_history,
    get_device_info,
    init_database,
    load_model,
    process_image_analysis,
    validate_image,
)


st.set_page_config(
    page_title="åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded",
)


def _ensure_session_state(available_models: Dict[str, Dict[str, str]]) -> None:
    """åˆå§‹åŒ– Streamlit session çŠ¶æ€ã€‚"""
    defaults = {
        "model_loaded": False,
        "model": None,
        "processor": None,
        "device": None,
        "model_type": "glm4v",
        "analysis_results": [],
        "loading_model": False,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

    st.session_state.available_models = available_models

    if not available_models:
        st.session_state.current_model = None
        st.session_state.model_loaded = False
        return

    if (
        "current_model" not in st.session_state
        or st.session_state.current_model not in available_models
    ):
        st.session_state.current_model = next(iter(available_models.keys()))
        st.session_state.model_loaded = False


def _register_analysis_result(
    filename: str,
    image: Image.Image,
    result: AnalysisResult,
) -> Dict[str, str]:
    """å°†åˆ†æç»“æœå­˜å…¥ session_stateï¼Œä¾¿äºå†å²è®°å½•å±•ç¤ºã€‚"""
    record = {
        "filename": filename,
        "timestamp": result.created_at,
        "image_size": f"{image.size[0]}x{image.size[1]}",
        "processing_time": f"{result.processing_time:.2f}s",
        "raw_result": result.raw_result,
        "think_content": result.think_content,
        "answer_content": result.answer_content,
        "status": "success",
        "from_cache": result.from_cache,
        "model_name": st.session_state.current_model,
    }
    st.session_state.analysis_results.append(record)
    return record


def _display_analysis_panels(result: AnalysisResult) -> None:
    """åœ¨ UI ä¸­å±•ç¤ºåˆ†æç»“æœã€‚"""
    st.markdown(f"**â±ï¸ å¤„ç†æ—¶é—´ï¼š** {result.processing_time:.2f} ç§’")

    if result.answer_content:
        with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True):
            st.write(result.answer_content)

    if result.think_content:
        with st.expander("ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False):
            st.write(result.think_content)

    if not result.answer_content and not result.think_content:
        st.write("**è¯Šæ–­ç»“æœï¼š**")
        st.write(result.raw_result)


def _build_download_report(
    filename: str, image: Image.Image, result: AnalysisResult
) -> str:
    """æ„å»ºä¸‹è½½æŠ¥å‘Šæ–‡æœ¬ã€‚"""
    report = ["åŒ»å­¦å›¾åƒè¯Šæ–­æŠ¥å‘Š", "=" * 50]
    report.append(f"æ–‡ä»¶åï¼š{filename}")
    report.append(f"ç”Ÿæˆæ—¶é—´ï¼š{result.created_at}")
    if st.session_state.current_model:
        report.append(f"ä½¿ç”¨æ¨¡å‹ï¼š{st.session_state.current_model}")
    report.append(f"å¤„ç†æ—¶é—´ï¼š{result.processing_time:.2f} ç§’")
    report.append(f"å›¾åƒå°ºå¯¸ï¼š{image.size[0]}x{image.size[1]} åƒç´ \n")

    if result.answer_content:
        report.append("è¯Šæ–­ç»“æœï¼š")
        report.append("-" * 30)
        report.append(result.answer_content)
        report.append("")

    if result.think_content:
        report.append("åˆ†æè¿‡ç¨‹ï¼š")
        report.append("-" * 30)
        report.append(result.think_content)
        report.append("")

    if not result.answer_content and not result.think_content:
        report.append("è¯Šæ–­ç»“æœï¼š")
        report.append(result.raw_result)

    return "\n".join(report)


def _create_stream_callback(placeholder):
    """æ„å»ºç”¨äºæµå¼å±•ç¤ºçš„å›è°ƒã€‚"""
    buffer: List[str] = []

    def _callback(chunk: str) -> None:
        buffer.append(chunk)
        placeholder.write("".join(buffer))

    return _callback, buffer


def _ensure_card_style() -> None:
    st.markdown(
        """
        <style>
        .info-card {
            background: rgba(255, 255, 255, 0.08);
            border: 1px solid rgba(140, 140, 140, 0.18);
            border-radius: 10px;
            padding: 0.65rem 0.85rem;
            margin-bottom: 0.6rem;
        }
        .info-card .info-label {
            color: var(--secondary-text-color, #5e6a7d);
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.04em;
            margin-bottom: 0.1rem;
        }
        .info-card .info-value {
            font-size: 1.08rem;
            font-weight: 600;
            color: var(--text-color, inherit);
            word-break: break-word;
        }
        .info-card .info-help {
            color: var(--secondary-text-color, #7a8796);
            font-size: 0.72rem;
            margin-top: 0.25rem;
        }
        .info-card code {
            background: rgba(255,255,255,0.12);
            padding: 0.05rem 0.35rem;
            border-radius: 4px;
            font-size: 0.82rem;
            display: inline-block;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )


def _render_info_cards(items: List[Dict[str, str]], columns: int = 1) -> None:
    _ensure_card_style()

    if not items:
        st.info("æš‚æ— ä¿¡æ¯")
        return

    for start in range(0, len(items), columns):
        current = items[start : start + columns]
        cols = st.columns(len(current))
        for col, item in zip(cols, current):
            value = str(item.get("value", "-"))
            help_text = item.get("help", "")
            col.markdown(
                f"""
                <div class="info-card">
                    <div class="info-label">{item.get("label", "")}</div>
                    <div class="info-value">{value}</div>
                    {f'<div class="info-help">{help_text}</div>' if help_text else ""}
                </div>
                """,
                unsafe_allow_html=True,
            )


def _load_selected_model(selected_model: str, model_config: Dict[str, str]) -> None:
    """åŠ è½½é€‰å®šçš„æ¨¡å‹å¹¶æ›´æ–° session çŠ¶æ€ã€‚"""
    with st.spinner(f"æ­£åœ¨åŠ è½½ {selected_model} æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
        model, processor, device, model_type = load_model(selected_model, model_config)

    st.session_state.model = model
    st.session_state.processor = processor
    st.session_state.device = device
    st.session_state.current_model = selected_model
    st.session_state.model_type = model_type
    st.session_state.model_loaded = True
    st.session_state.loading_model = False
    st.session_state.pop("loading_target", None)
    st.success(f"âœ… {selected_model} æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    st.rerun()


def _render_device_info_section() -> None:
    device_info = get_device_info()
    _render_info_cards(device_info, columns=1)


def _render_sidebar() -> None:
    """æ¸²æŸ“ä¾§è¾¹æ é…ç½®ã€‚"""
    available_models = st.session_state.available_models

    def _format_model_option(name: str) -> str:
        info = available_models[name]
        if info.get("source") == "local":
            identifier = info.get("identifier", name)
            if "/" in identifier:
                owner, model = identifier.split("/", 1)
                return f"{owner} / {model}"
            return identifier
        return info.get("description", name)

    with st.sidebar:
        st.header("ğŸ”§ ç³»ç»Ÿé…ç½®")

        st.subheader("ğŸ’» è®¾å¤‡ä¿¡æ¯")
        _render_device_info_section()

        st.divider()

        st.subheader("ğŸ¤– æ¨¡å‹")

        if not available_models:
            st.error("âš ï¸ æœªå‘ç°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é»˜è®¤ç›®å½•æˆ–ç½‘ç»œé…ç½®")
            return

        model_names = list(available_models.keys())
        default_index = (
            model_names.index(st.session_state.current_model)
            if st.session_state.current_model in model_names
            else 0
        )

        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=model_names,
            index=default_index,
            format_func=_format_model_option,
        )

        model_info = available_models[selected_model]
        st.caption(f"æ¨¡å‹ä½ç½®ï¼š{model_info.get('path')}")

        model_needs_reload = (
            not st.session_state.model_loaded
            or st.session_state.current_model != selected_model
        )

        is_loading = st.session_state.get("loading_model", False)
        loading_target = st.session_state.get("loading_target")

        if is_loading and loading_target and loading_target != selected_model:
            st.session_state.loading_model = False
            st.session_state.pop("loading_target", None)
            st.info("å·²åˆ‡æ¢æ¨¡å‹ï¼Œå–æ¶ˆå…ˆå‰çš„åŠ è½½ä»»åŠ¡")
            is_loading = False

        button_label = "â³ æ­£åœ¨åŠ è½½...ç‚¹å‡»å–æ¶ˆ" if is_loading else (
            "ğŸš€ åŠ è½½æ¨¡å‹" if model_needs_reload else "ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹"
        )

        button_clicked = st.button(
            button_label,
            key="load_model_button",
            width="stretch",
            type="primary",
        )

        if button_clicked:
            if is_loading:
                st.session_state.loading_model = False
                st.session_state.pop("loading_target", None)
                st.info("å·²å–æ¶ˆæ¨¡å‹åŠ è½½")
            else:
                st.session_state.loading_model = True
                st.session_state.loading_target = selected_model
                try:
                    _load_selected_model(selected_model, model_info)
                except Exception as exc:
                    st.session_state.loading_model = False
                    st.session_state.pop("loading_target", None)
                    st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{exc}")

        if st.session_state.model_loaded and (
            st.session_state.current_model == selected_model
        ):
            st.success(f"{selected_model} å·²åŠ è½½")
        else:
            st.warning("æ¨¡å‹å°šæœªåŠ è½½")

        st.divider()


def _render_single_analysis_tab() -> None:
    st.header("å•å¼ å›¾åƒåˆ†æ")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©åŒ»å­¦å›¾åƒæ–‡ä»¶",
        type=[ext.strip(".") for ext in SUPPORTED_FORMATS],
        help="æ”¯æŒ JPG/JPEG/PNG/BMP/TIFF/WebPï¼Œå¤§å°é™åˆ¶ 10MB",
    )

    if uploaded_file is None:
        return

    is_valid, message = validate_image(uploaded_file)
    if not is_valid:
        st.error(f"âŒ {message}")
        return

    st.success(f"âœ… {message}")

    col_image, col_action = st.columns([1, 1])
    with col_image:
        st.subheader("ğŸ“· åŸå§‹å›¾åƒ")
        uploaded_file.seek(0)
        with Image.open(uploaded_file) as img:
            image = img.convert("RGB")
        st.image(
            image,
            caption=f"æ–‡ä»¶å: {uploaded_file.name}",
            width="stretch",
        )

        st.markdown(
            f"""
            <div class="image-info">
            ğŸ“ <strong>å°ºå¯¸ï¼š</strong> {image.size[0]} Ã— {image.size[1]} åƒç´ <br>
            ğŸ¨ <strong>æ¨¡å¼ï¼š</strong> {image.mode}<br>
            ğŸ“ <strong>æ ¼å¼ï¼š</strong> {uploaded_file.type or "N/A"}<br>
            ğŸ“ <strong>å¤§å°ï¼š</strong> {uploaded_file.size / 1024:.1f} KB
            </div>
            """,
            unsafe_allow_html=True,
        )

    with col_action:
        st.subheader("ğŸ¤– AIè¯Šæ–­åˆ†æ")
        start_analysis = st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary")
        streaming_placeholder = st.empty()

        if start_analysis:
            if not st.session_state.model_loaded:
                st.error("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½æ¨¡å‹")
                return

            stream_callback, stream_buffer = _create_stream_callback(
                streaming_placeholder
            )

            try:
                analysis_result = process_image_analysis(
                    image=image,
                    model=st.session_state.model,
                    processor=st.session_state.processor,
                    device=st.session_state.device,
                    model_type=st.session_state.model_type,
                    model_name=st.session_state.current_model,
                    image_name=uploaded_file.name,
                    prompt=DEFAULT_MEDICAL_PROMPT,
                    enable_cache=False,
                    stream_callback=stream_callback,
                )
            except Exception as exc:
                streaming_placeholder.empty()
                st.error(f"âŒ åˆ†æå¤±è´¥ï¼š{exc}")
                return

            streaming_placeholder.empty()

            if analysis_result.from_cache:
                st.info("ğŸ¯ å‘½ä¸­ç¼“å­˜ï¼šå·²åŠ è½½å†å²åˆ†æç»“æœ")

            _display_analysis_panels(analysis_result)
            _register_analysis_result(uploaded_file.name, image, analysis_result)

            download_data = _build_download_report(
                uploaded_file.name, image, analysis_result
            )
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½è¯Šæ–­æŠ¥å‘Š",
                data=download_data,
                file_name=f"{os.path.splitext(uploaded_file.name)[0]}_è¯Šæ–­æŠ¥å‘Š.txt",
                mime="text/plain",
            )


def _render_batch_analysis_tab() -> None:
    st.header("æ‰¹é‡å›¾åƒå¤„ç†")
    uploaded_files = st.file_uploader(
        "é€‰æ‹©å¤šå¼ åŒ»å­¦å›¾åƒæ–‡ä»¶",
        type=[ext.strip(".") for ext in SUPPORTED_FORMATS],
        accept_multiple_files=True,
        help="å¯ä»¥åŒæ—¶ä¸Šä¼ å¤šå¼ å›¾åƒè¿›è¡Œæ‰¹é‡åˆ†æ",
    )

    if not uploaded_files:
        return

    st.success(f"âœ… å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
    with st.expander("ğŸ“ æ–‡ä»¶åˆ—è¡¨", expanded=True):
        for index, file in enumerate(uploaded_files, start=1):
            st.write(f"{index}. {file.name} ({file.size / 1024:.1f} KB)")

    if not st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ", type="primary"):
        return

    if not st.session_state.model_loaded:
        st.error("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½æ¨¡å‹")
        return

    overall_progress = st.progress(0.0)
    status_placeholder = st.empty()
    results_container = st.container()

    batch_results: List[Dict[str, str]] = []
    total_start_time = time.time()

    for index, uploaded_file in enumerate(uploaded_files, start=1):
        progress = (index - 1) / len(uploaded_files)
        overall_progress.progress(progress)
        status_placeholder.write(
            f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {index}/{len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼š{uploaded_file.name}"
        )

        is_valid, message = validate_image(uploaded_file)
        if not is_valid:
            warning_message = f"âš ï¸ {uploaded_file.name}: {message}"
            batch_results.append(
                {
                    "filename": uploaded_file.name,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "status": "invalid",
                    "error": message,
                    "model_name": st.session_state.current_model,
                }
            )
            with results_container:
                st.warning(warning_message)
            continue

        uploaded_file.seek(0)
        with Image.open(uploaded_file) as img:
            image = img.convert("RGB")

        try:
            analysis_result = process_image_analysis(
                image=image,
                model=st.session_state.model,
                processor=st.session_state.processor,
                device=st.session_state.device,
                model_type=st.session_state.model_type,
                model_name=st.session_state.current_model,
                image_name=uploaded_file.name,
                prompt=DEFAULT_MEDICAL_PROMPT,
                enable_cache=True,
                stream_callback=None,
            )
        except Exception as exc:
            error_message = f"âŒ {uploaded_file.name} å¤„ç†å¤±è´¥ï¼š{exc}"
            batch_results.append(
                {
                    "filename": uploaded_file.name,
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "status": "error",
                    "error": str(exc),
                    "model_name": st.session_state.current_model,
                }
            )
            with results_container:
                st.error(error_message)
            continue

        record = _register_analysis_result(uploaded_file.name, image, analysis_result)
        batch_results.append(record)

        with results_container:
            header = f"âœ… {uploaded_file.name} - åˆ†æå®Œæˆ ({analysis_result.processing_time:.2f}s)"
            if analysis_result.from_cache:
                header += " [ç¼“å­˜]"
            with st.expander(header, expanded=False):
                col_img, col_result = st.columns([1, 2])
                with col_img:
                    st.image(
                        image,
                        caption=uploaded_file.name,
                        width="stretch",
                    )
                with col_result:
                    _display_analysis_panels(analysis_result)

    overall_progress.progress(1.0)
    total_time = time.time() - total_start_time
    status_placeholder.success(
        f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’ï¼ˆå¹³å‡æ¯å¼  {total_time / max(len(uploaded_files), 1):.2f} ç§’ï¼‰"
    )

    if batch_results:
        report_lines = [
            "æ‰¹é‡åŒ»å­¦å›¾åƒåˆ†ææŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"æ€»æ–‡ä»¶æ•°ï¼š{len(uploaded_files)}",
            f"æ€»å¤„ç†æ—¶é—´ï¼š{total_time:.2f} ç§’",
            f"å¹³å‡å¤„ç†æ—¶é—´ï¼š{total_time / max(len(uploaded_files), 1):.2f} ç§’",
            "=" * 80,
            "",
        ]

        for item in batch_results:
            report_lines.append(f"æ–‡ä»¶åï¼š{item['filename']}")
            report_lines.append(f"æ—¶é—´æˆ³ï¼š{item['timestamp']}")
            if item.get("model_name"):
                report_lines.append(f"æ¨¡å‹ï¼š{item['model_name']}")
            if item.get("status") == "success":
                report_lines.append(f"å›¾åƒå°ºå¯¸ï¼š{item.get('image_size', 'æœªçŸ¥')}")
                report_lines.append(f"å¤„ç†æ—¶é—´ï¼š{item.get('processing_time', 'N/A')}")
                report_lines.append("è¯Šæ–­ç»“æœï¼š")
                report_lines.append(item.get("raw_result", ""))
            else:
                report_lines.append(f"çŠ¶æ€ï¼š{item.get('status')}")
                report_lines.append(f"é”™è¯¯ï¼š{item.get('error', 'æœªçŸ¥é”™è¯¯')}")
            report_lines.append("\n" + "-" * 80 + "\n")

        st.download_button(
            label="ğŸ“¦ ä¸‹è½½æ‰¹é‡åˆ†ææŠ¥å‘Š",
            data="\n".join(report_lines),
            file_name=f"æ‰¹é‡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
        )


def _render_history_tab() -> None:
    st.header("å†å²åˆ†æç»“æœ")
    results = st.session_state.analysis_results

    if not results:
        st.info("ğŸ“­ æš‚æ— å†å²åˆ†æç»“æœ")
        return

    total_analyses = len(results)
    successful = sum(1 for item in results if item.get("status") == "success")
    success_rate = successful / total_analyses * 100 if total_analyses else 0.0

    col1, col2, col3 = st.columns(3)
    col1.metric("æ€»åˆ†ææ¬¡æ•°", total_analyses)
    col2.metric("æˆåŠŸåˆ†æ", successful)
    col3.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")

    st.divider()

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²è®°å½•"):
        st.session_state.analysis_results = []
        st.rerun()

    for item in reversed(results):
        header = f"ğŸ“„ {item['filename']} - {item['timestamp']}"
        if item.get("from_cache"):
            header += " [ç¼“å­˜]"
        with st.expander(header, expanded=False):
            st.write(f"**æ–‡ä»¶åï¼š** {item['filename']}")
            st.write(f"**æ—¶é—´ï¼š** {item['timestamp']}")
            if item.get("model_name"):
                st.write(f"**æ¨¡å‹ï¼š** {item['model_name']}")
            if item.get("image_size"):
                st.write(f"**å°ºå¯¸ï¼š** {item['image_size']}")
            if item.get("processing_time"):
                st.write(f"**å¤„ç†æ—¶é—´ï¼š** {item['processing_time']}")

            if item.get("answer_content"):
                with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True):
                    st.write(item["answer_content"])

            if item.get("think_content"):
                with st.expander("ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False):
                    st.write(item["think_content"])

            if not item.get("answer_content") and not item.get("think_content"):
                st.write("**è¯Šæ–­ç»“æœï¼š**")
                st.write(item.get("raw_result", ""))


def _render_cache_tab() -> None:
    st.header("ç¼“å­˜ç®¡ç†")

    st.markdown(
        """
        ### ğŸ—ƒï¸ æ•°æ®åº“ç¼“å­˜ç³»ç»Ÿ
        - ğŸ” **å›¾ç‰‡è¯†åˆ«**ï¼šåŸºäºå›¾ç‰‡å†…å®¹çš„ MD5 å“ˆå¸Œå€¼
        - ğŸ“ **æç¤ºè¯åŒ¹é…**ï¼šåŸºäºæç¤ºè¯å†…å®¹çš„ MD5 å“ˆå¸Œå€¼
        - ğŸ¤– **æ¨¡å‹åŒºåˆ†**ï¼šä¸åŒæ¨¡å‹çš„ç»“æœç‹¬ç«‹ç¼“å­˜
        - âš¡ **å¿«é€ŸæŸ¥è¯¢**ï¼šSQLite ç´¢å¼•ä¼˜åŒ–æ£€ç´¢æ€§èƒ½
        """
    )

    try:
        metrics = fetch_cache_metrics()
    except Exception as exc:
        st.error(f"âŒ è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥ï¼š{exc}")
        return

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("æ€»ç¼“å­˜è®°å½•", metrics["total_records"])
    col2.metric("è¿‘7å¤©è®°å½•", metrics["recent_records"])
    col3.metric("æ•°æ®åº“å¤§å°", f"{metrics['db_size_mb']:.2f} MB")
    col4.metric("ç¼“å­˜æ–‡ä»¶", "medical_analysis_cache.db")

    if metrics["model_stats"]:
        st.subheader("ğŸ“Š æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡")
        for model_name, count in metrics["model_stats"]:
            st.write(f"**{model_name}**ï¼š{count} æ¬¡åˆ†æ")

    st.divider()
    st.subheader("ğŸ“‹ ç¼“å­˜å†å²è®°å½•")

    record_limit = st.selectbox("æ˜¾ç¤ºè®°å½•æ•°", [10, 25, 50, 100], index=1)
    try:
        history_records = get_analysis_history(record_limit)
    except Exception as exc:
        st.error(f"âŒ è·å–å†å²è®°å½•å¤±è´¥ï¼š{exc}")
        history_records = []

    if history_records:
        for record in history_records:
            header = f"ğŸ“„ {record['image_name']} - {record['created_at'][:19]}"
            with st.expander(header, expanded=False):
                st.write(f"**æ–‡ä»¶åï¼š** {record['image_name']}")
                st.write(f"**å›¾ç‰‡å°ºå¯¸ï¼š** {record['image_size']}")
                st.write(f"**æ¨¡å‹ï¼š** {record['model_name']} ({record['model_type']})")
                st.write(f"**å¤„ç†æ—¶é—´ï¼š** {record['processing_time']:.2f} ç§’")
                st.write(f"**ç¼“å­˜æ—¶é—´ï¼š** {record['created_at'][:19]}")

                if record.get("answer_content"):
                    with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True):
                        st.write(record["answer_content"])
                if record.get("think_content"):
                    with st.expander("ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False):
                        st.write(record["think_content"])
    else:
        st.info("ğŸ“­ æš‚æ— ç¼“å­˜è®°å½•")

    st.divider()
    st.subheader("ğŸ› ï¸ ç¼“å­˜æ“ä½œ")

    col_clear, col_export = st.columns(2)

    with col_clear:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç¼“å­˜", type="secondary"):
            try:
                clear_analysis_cache()
            except Exception as exc:
                st.error(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥ï¼š{exc}")
            else:
                st.success("âœ… ç¼“å­˜å·²æ¸…ç©º")
                st.rerun()

    with col_export:
        if st.button("ğŸ“¤ å¯¼å‡ºç¼“å­˜æ•°æ®"):
            try:
                import pandas as pd

                records = fetch_all_cache_records()
                if not records:
                    st.info("ğŸ“­ æš‚æ— ç¼“å­˜æ•°æ®å¯å¯¼å‡º")
                else:
                    df = pd.DataFrame(records)
                    csv_data = df.to_csv(index=False)
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½CSVæ–‡ä»¶",
                        data=csv_data,
                        file_name=f"analysis_cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                    )
                    st.success("âœ… å¯¼å‡ºå‡†å¤‡å®Œæˆ")
            except ImportError:
                st.warning("âš ï¸ éœ€è¦å®‰è£… pandas åº“æ‰èƒ½å¯¼å‡º CSV")
            except Exception as exc:
                st.error(f"âŒ å¯¼å‡ºå¤±è´¥ï¼š{exc}")


def main() -> None:
    init_database()
    available_models = build_available_models()
    _ensure_session_state(available_models)
    st.markdown(
        '<h1 class="main-header">ğŸ¥ åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ</h1>', unsafe_allow_html=True
    )
    _render_sidebar()

    if not st.session_state.available_models:
        st.warning("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æœ¬åœ°ç¼“å­˜ç›®å½•æˆ–ç½‘ç»œé…ç½®ã€‚")
        return

    if not st.session_state.model_loaded or not st.session_state.current_model:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½æ¨¡å‹")
        return

    tab_single, tab_batch, tab_history, tab_cache = st.tabs(
        ["ğŸ–¼ï¸ å•å¼ å›¾åƒåˆ†æ", "ğŸ“Š æ‰¹é‡å¤„ç†", "ğŸ“‹ å†å²ç»“æœ", "ğŸ—ƒï¸ ç¼“å­˜ç®¡ç†"]
    )

    with tab_single:
        _render_single_analysis_tab()

    with tab_batch:
        _render_batch_analysis_tab()

    with tab_history:
        _render_history_tab()

    with tab_cache:
        _render_cache_tab()


if __name__ == "__main__":
    main()

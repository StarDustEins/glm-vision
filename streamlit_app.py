import streamlit as st
import torch
import time
import os
from PIL import Image
from modelscope import AutoProcessor, Glm4vForConditionalGeneration
from transformers import TextIteratorStreamer
import threading
from datetime import datetime
import sqlite3
import hashlib
import re

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded",
)

# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
SUPPORTED_FORMATS = [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"]

# æ•°æ®åº“é…ç½®
DATABASE_PATH = "medical_analysis_cache.db"

# é»˜è®¤ä¸“ä¸šåŒ»å­¦è¯Šæ–­æç¤ºè¯
DEFAULT_MEDICAL_PROMPT = """
ä½œä¸ºèµ„æ·±åŒ»å­¦å½±åƒè¯Šæ–­ä¸“å®¶ï¼Œè¯·å¯¹è¿™å¼ åŒ»å­¦å›¾åƒè¿›è¡Œå…¨é¢ã€ç³»ç»Ÿçš„ä¸“ä¸šåˆ†æã€‚

**åˆ†ææ¡†æ¶ï¼š**

<think>
è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œç³»ç»Ÿæ€§åˆ†ææ€è€ƒï¼š

1. **æŠ€æœ¯è´¨é‡è¯„ä¼°**
   - å›¾åƒé‡‡é›†æŠ€æœ¯å‚æ•°ï¼ˆæ‰«ææ–¹å¼ã€å±‚åšã€å¯¹æ¯”å‰‚ç­‰ï¼‰
   - å›¾åƒè´¨é‡è¯„ä»·ï¼ˆæ¸…æ™°åº¦ã€ä¼ªå½±ã€ä½“ä½ç­‰ï¼‰
   - è§£å‰–ç»“æ„æ˜¾ç¤ºå®Œæ•´æ€§

2. **è§£å‰–ç»“æ„ç³»ç»Ÿè¯„ä¼°**
   - éª¨éª¼ç³»ç»Ÿï¼šéª¨è´¨å¯†åº¦ã€éª¨çš®è´¨è¿ç»­æ€§ã€å…³èŠ‚é—´éš™
   - è½¯ç»„ç»‡ï¼šè‚Œè‚‰ã€è„‚è‚ªå±‚æ¬¡ã€ç­‹è†œå¹³é¢
   - å™¨å®˜å½¢æ€ï¼šå¤§å°ã€å½¢çŠ¶ã€ä½ç½®ã€å¯†åº¦/ä¿¡å·
   - è¡€ç®¡ç³»ç»Ÿï¼šä¸»è¦è¡€ç®¡èµ°è¡Œã€ç®¡è…”æƒ…å†µ

3. **å¼‚å¸¸å‘ç°è¯¦ç»†åˆ†æ**
   - ç—…å˜ä½ç½®ï¼šç²¾ç¡®è§£å‰–å®šä½
   - å½¢æ€ç‰¹å¾ï¼šå¤§å°ã€å½¢çŠ¶ã€è¾¹ç•Œã€å†…éƒ¨ç»“æ„
   - å¯†åº¦/ä¿¡å·ç‰¹å¾ï¼šCTå€¼ã€T1/T2ä¿¡å·ç‰¹ç‚¹
   - å¼ºåŒ–æ¨¡å¼ï¼šå¯¹æ¯”å‰‚å¼ºåŒ–ç‰¹å¾ï¼ˆå¦‚é€‚ç”¨ï¼‰
   - å‘¨å›´ç»„ç»‡å…³ç³»ï¼šæµ¸æ¶¦ã€å‹è¿«ã€ç§»ä½

4. **å½±åƒå­¦æµ‹é‡**
   - ç—…å˜æœ€å¤§å¾„çº¿æµ‹é‡
   - å¤šå¹³é¢æµ‹é‡æ•°æ®
   - ä½“ç§¯è¯„ä¼°ï¼ˆå¦‚éœ€è¦ï¼‰

5. **é‰´åˆ«è¯Šæ–­æ€è·¯**
   - ä¸»è¦è¯Šæ–­è€ƒè™‘åŠä¾æ®
   - éœ€è¦æ’é™¤çš„ç–¾ç—…
   - è¯Šæ–­ç½®ä¿¡åº¦è¯„ä¼°
</think>

<answer>
**å½±åƒå­¦æŠ¥å‘Š**

**æŠ€æœ¯å‚æ•°ï¼š**
[æè¿°æ‰«ææŠ€æœ¯å‚æ•°å’Œå›¾åƒè´¨é‡]

**å½±åƒå­¦æ‰€è§ï¼š**
[æŒ‰è§£å‰–ç³»ç»Ÿç³»ç»Ÿæ€§æè¿°æ­£å¸¸å’Œå¼‚å¸¸å‘ç°]

**æµ‹é‡æ•°æ®ï¼š**
[æä¾›ç²¾ç¡®çš„æµ‹é‡ç»“æœ]

**å½±åƒå­¦å°è±¡ï¼š**
1. ä¸»è¦è¯Šæ–­è€ƒè™‘
2. é‰´åˆ«è¯Šæ–­
3. å»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥
4. éšè®¿å»ºè®®

**å¤‡æ³¨ï¼š**
[é‡è¦æé†’å’Œæ³¨æ„äº‹é¡¹]
</answer>

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼ä½¿ç”¨ä¸­æ–‡è¾“å‡ºï¼Œç¡®ä¿åˆ†æçš„ä¸“ä¸šæ€§å’Œç³»ç»Ÿæ€§ã€‚
"""

# æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    "GLM-4V-9B": {
        "path": "ZhipuAI/GLM-4.1V-9B-Thinking",
        "description": "GLM-4Vå¤šæ¨¡æ€å¤§æ¨¡å‹ (9Bå‚æ•°)",
        "type": "multimodal",
    },
    "MedGemma-4B": {
        "path": r"C:\Users\Hi\.cache\modelscope\hub\models\google\medgemma-4b-it",
        "description": "MedGemmaåŒ»å­¦ä¸“ç”¨æ¨¡å‹ (4Bå‚æ•°)",
        "type": "medical",
    },
}

# åˆå§‹åŒ–session state
if "model_loaded" not in st.session_state:
    st.session_state.model_loaded = False
if "model" not in st.session_state:
    st.session_state.model = None
if "processor" not in st.session_state:
    st.session_state.processor = None
if "device" not in st.session_state:
    st.session_state.device = None
if "current_model" not in st.session_state:
    st.session_state.current_model = "GLM-4V-9B"
if "model_type" not in st.session_state:
    st.session_state.model_type = "glm4v"
if "analysis_results" not in st.session_state:
    st.session_state.analysis_results = []


# æ•°æ®åº“æ“ä½œå‡½æ•°
def init_database():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # åˆ›å»ºåˆ†æç»“æœè¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS analysis_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            image_hash TEXT UNIQUE NOT NULL,
            image_name TEXT NOT NULL,
            image_size TEXT NOT NULL,
            model_type TEXT NOT NULL,
            model_name TEXT NOT NULL,
            prompt_hash TEXT NOT NULL,
            raw_result TEXT NOT NULL,
            think_content TEXT,
            answer_content TEXT,
            processing_time REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_image_hash ON analysis_results(image_hash)
    """)
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_created_at ON analysis_results(created_at)
    """)

    conn.commit()
    conn.close()


def calculate_image_hash(image):
    """è®¡ç®—å›¾ç‰‡çš„MD5å“ˆå¸Œå€¼"""
    # å°†PILå›¾åƒè½¬æ¢ä¸ºå­—èŠ‚
    import io

    img_byte_arr = io.BytesIO()
    image.save(img_byte_arr, format="PNG")
    img_byte_arr = img_byte_arr.getvalue()

    # è®¡ç®—MD5å“ˆå¸Œ
    return hashlib.md5(img_byte_arr).hexdigest()


def calculate_prompt_hash(prompt):
    """è®¡ç®—æç¤ºè¯çš„MD5å“ˆå¸Œå€¼"""
    return hashlib.md5(prompt.encode("utf-8")).hexdigest()


def save_analysis_result(
    image,
    image_name,
    model_type,
    model_name,
    prompt,
    raw_result,
    think_content,
    answer_content,
    processing_time,
):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“"""
    try:
        image_hash = calculate_image_hash(image)
        prompt_hash = calculate_prompt_hash(prompt)
        image_size = f"{image.size[0]}x{image.size[1]}"

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT OR REPLACE INTO analysis_results 
            (image_hash, image_name, image_size, model_type, model_name, prompt_hash,
             raw_result, think_content, answer_content, processing_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                image_hash,
                image_name,
                image_size,
                model_type,
                model_name,
                prompt_hash,
                raw_result,
                think_content,
                answer_content,
                processing_time,
            ),
        )

        conn.commit()
        conn.close()
        return True
    except Exception as e:
        st.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥ï¼š{e}")
        return False


def get_cached_result(image, model_type, model_name, prompt):
    """ä»æ•°æ®åº“è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
    try:
        image_hash = calculate_image_hash(image)
        prompt_hash = calculate_prompt_hash(prompt)

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT raw_result, think_content, answer_content, processing_time, created_at
            FROM analysis_results 
            WHERE image_hash = ? AND model_type = ? AND model_name = ? AND prompt_hash = ?
            ORDER BY created_at DESC LIMIT 1
        """,
            (image_hash, model_type, model_name, prompt_hash),
        )

        result = cursor.fetchone()
        conn.close()

        if result:
            return {
                "raw_result": result[0],
                "think_content": result[1],
                "answer_content": result[2],
                "processing_time": result[3],
                "created_at": result[4],
                "from_cache": True,
            }
        return None
    except Exception as e:
        st.error(f"æŸ¥è¯¢ç¼“å­˜å¤±è´¥ï¼š{e}")
        return None


def get_analysis_history(limit=50):
    """è·å–åˆ†æå†å²è®°å½•"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT image_name, image_size, model_type, model_name, 
                   think_content, answer_content, processing_time, created_at
            FROM analysis_results 
            ORDER BY created_at DESC LIMIT ?
        """,
            (limit,),
        )

        results = cursor.fetchall()
        conn.close()

        return [
            {
                "image_name": row[0],
                "image_size": row[1],
                "model_type": row[2],
                "model_name": row[3],
                "think_content": row[4],
                "answer_content": row[5],
                "processing_time": row[6],
                "created_at": row[7],
            }
            for row in results
        ]
    except Exception as e:
        st.error(f"è·å–å†å²è®°å½•å¤±è´¥ï¼š{e}")
        return []


def clear_analysis_cache():
    """æ¸…ç©ºåˆ†æç¼“å­˜"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM analysis_results")
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        st.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥ï¼š{e}")
        return False


def update_processing_time(image, model_type, model_name, prompt, processing_time):
    """æ›´æ–°æ•°æ®åº“ä¸­çš„å¤„ç†æ—¶é—´"""
    try:
        image_hash = calculate_image_hash(image)
        prompt_hash = calculate_prompt_hash(prompt)

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        cursor.execute(
            """
            UPDATE analysis_results 
            SET processing_time = ?
            WHERE image_hash = ? AND model_type = ? AND model_name = ? AND prompt_hash = ?
        """,
            (processing_time, image_hash, model_type, model_name, prompt_hash),
        )

        conn.commit()
        conn.close()
        return True
    except Exception as e:
        st.error(f"æ›´æ–°å¤„ç†æ—¶é—´å¤±è´¥ï¼š{e}")
        return False


# åˆå§‹åŒ–æ•°æ®åº“
init_database()


@st.cache_resource
def load_model(model_name):
    """åŠ è½½æŒ‡å®šçš„æ¨¡å‹ï¼ˆç¼“å­˜ä»¥é¿å…é‡å¤åŠ è½½ï¼‰"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    if model_name not in AVAILABLE_MODELS:
        st.error(f"æœªçŸ¥æ¨¡å‹ï¼š{model_name}")
        return None, None, device, None

    model_config = AVAILABLE_MODELS[model_name]
    model_path = model_config["path"]

    try:
        if model_name == "GLM-4V-9B":
            # GLM-4Væ¨¡å‹åŠ è½½
            model = Glm4vForConditionalGeneration.from_pretrained(
                pretrained_model_name_or_path=model_path,
                torch_dtype=torch.bfloat16,
                device_map="cuda:0" if torch.cuda.is_available() else "cpu",
            )
            processor = AutoProcessor.from_pretrained(model_path, use_fast=True)
            return model, processor, device, "glm4v"

        elif model_name == "MedGemma-4B":
            # MedGemmaæ¨¡å‹ä½¿ç”¨transformersåŠ è½½
            from transformers import AutoModelForCausalLM
            from transformers import AutoProcessor as MedGemmaProcessor

            # æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path):
                # å¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨çº¿åŠ è½½
                model_path = "google/medgemma-4b-it"

            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="cuda:0" if torch.cuda.is_available() else "cpu",
                trust_remote_code=True,
            )
            processor = MedGemmaProcessor.from_pretrained(
                model_path, trust_remote_code=True
            )
            return model, processor, device, "medgemma"

    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        return None, None, device, None


def get_device_info():
    """è·å–æ ¸å¿ƒè®¾å¤‡ä¿¡æ¯"""
    import psutil
    
    device_info = []
    
    # GPUä¿¡æ¯
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = f"{torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB"
        device_info.append({"é¡¹ç›®": "ğŸ® GPU", "ä¿¡æ¯": f"{gpu_name} ({gpu_memory})"})
        device_info.append({"é¡¹ç›®": "ğŸ”§ CUDA", "ä¿¡æ¯": f"âœ… {torch.version.cuda}" if torch.version.cuda else "âœ… å¯ç”¨"})
    else:
        device_info.append({"é¡¹ç›®": "ğŸ® GPU", "ä¿¡æ¯": "âŒ ä¸å¯ç”¨ (ä½¿ç”¨CPU)"})
    
    # CPUä¿¡æ¯
    cpu_count = psutil.cpu_count(logical=True)
    cpu_usage = psutil.cpu_percent(interval=1)
    device_info.append({"é¡¹ç›®": "ğŸ’» CPU", "ä¿¡æ¯": f"{cpu_count} æ ¸å¿ƒ ({cpu_usage:.1f}% ä½¿ç”¨ä¸­)"})
    
    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    memory_total = memory.total / (1024**3)
    memory_usage = memory.percent
    device_info.append({"é¡¹ç›®": "ğŸ§  å†…å­˜", "ä¿¡æ¯": f"{memory_total:.1f} GB ({memory_usage:.1f}% ä½¿ç”¨ä¸­)"})
    
    return device_info


def validate_image(uploaded_file):
    """éªŒè¯ä¸Šä¼ çš„å›¾åƒæ–‡ä»¶"""
    if uploaded_file is None:
        return False, "æœªé€‰æ‹©æ–‡ä»¶"

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_ext = os.path.splitext(uploaded_file.name)[1].lower()
    if file_ext not in SUPPORTED_FORMATS:
        return (
            False,
            f"ä¸æ”¯æŒçš„æ ¼å¼ {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼ï¼š{', '.join(SUPPORTED_FORMATS)}",
        )

    # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ä¸º10MBï¼‰
    if uploaded_file.size > 10 * 1024 * 1024:
        return False, "æ–‡ä»¶å¤§å°è¶…è¿‡10MBé™åˆ¶"

    return True, "æ–‡ä»¶éªŒè¯é€šè¿‡"


def parse_analysis_result(raw_text):
    """è§£æåˆ†æç»“æœï¼Œåˆ†ç¦»thinkå’Œansweréƒ¨åˆ†"""

    # æŸ¥æ‰¾thinkå’Œansweræ ‡ç­¾
    think_match = re.search(r"<think>(.*?)</think>", raw_text, re.DOTALL)
    answer_match = re.search(r"<answer>(.*?)</answer>", raw_text, re.DOTALL)

    think_content = think_match.group(1).strip() if think_match else ""
    answer_content = answer_match.group(1).strip() if answer_match else ""

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºanswer
    if not think_content and not answer_content:
        answer_content = raw_text.strip()

    return think_content, answer_content


def process_image_analysis(
    image,
    model,
    processor,
    device,
    model_type="glm4v",
    output_container=None,
    image_name="unknown.jpg",
    model_name="Unknown",
    enable_cache=True,
):
    """å¤„ç†å›¾åƒåˆ†æï¼ˆç»Ÿä¸€æµå¼è¾“å‡ºï¼Œæ”¯æŒç¼“å­˜ï¼‰"""
    prompt = DEFAULT_MEDICAL_PROMPT

    # æ£€æŸ¥ç¼“å­˜
    if enable_cache:
        cached_result = get_cached_result(image, model_type, model_name, prompt)
        if cached_result:
            if output_container:
                with output_container:
                    st.success("ğŸ¯ **ä»ç¼“å­˜ä¸­è·å–ç»“æœï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰**")
                    st.info(f"ğŸ“… ç¼“å­˜æ—¶é—´ï¼š{cached_result['created_at']}")
                    st.markdown(
                        f"â±ï¸ **åŸå¤„ç†æ—¶é—´ï¼š** {cached_result['processing_time']:.2f} ç§’"
                    )

            return cached_result["raw_result"]

    try:
        if model_type == "glm4v":
            # GLM-4Væ¨¡å‹å¤„ç†æ–¹å¼
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": image,
                        },
                        {"type": "text", "text": prompt},
                    ],
                }
            ]

            # å‡†å¤‡è¾“å…¥
            inputs = processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
            ).to(device)

            # æµå¼ç”Ÿæˆ
            streamer = TextIteratorStreamer(
                processor.tokenizer, skip_prompt=True, skip_special_tokens=False
            )

            generation_kwargs = dict(
                inputs,
                max_new_tokens=8192,
                temperature=0.8,
                do_sample=True,
                top_p=0.9,
                streamer=streamer,
            )

            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
            thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
            thread.start()

            # æµå¼æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
            generated_text = ""
            if output_container:
                with output_container:
                    st.write("ğŸ¤– **AIæ­£åœ¨ç”Ÿæˆè¯Šæ–­ç»“æœ...**")
                    text_placeholder = st.empty()

                    for new_text in streamer:
                        generated_text += new_text
                        # å®æ—¶æ›´æ–°æ˜¾ç¤º
                        text_placeholder.write(generated_text)

            else:
                # å¦‚æœæ²¡æœ‰å®¹å™¨ï¼Œåªæ”¶é›†æ–‡æœ¬
                for new_text in streamer:
                    generated_text += new_text

            thread.join()
            return generated_text.strip()

        elif model_type == "medgemma":
            # MedGemmaæ¨¡å‹ä½¿ç”¨transformersæ–¹å¼å¤„ç†
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": image,
                        },
                        {"type": "text", "text": prompt},
                    ],
                }
            ]

            # å‡†å¤‡è¾“å…¥
            inputs = processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
            ).to(device)

            # æµå¼ç”Ÿæˆ
            streamer = TextIteratorStreamer(
                processor.tokenizer, skip_prompt=True, skip_special_tokens=False
            )

            generation_kwargs = dict(
                inputs,
                max_new_tokens=4096,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                streamer=streamer,
            )

            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
            thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
            thread.start()

            # æµå¼æ˜¾ç¤ºç”Ÿæˆçš„æ–‡æœ¬
            generated_text = ""
            if output_container:
                with output_container:
                    st.write("ğŸ¤– **MedGemmaæ­£åœ¨ç”Ÿæˆä¸“ä¸šè¯Šæ–­æŠ¥å‘Š...**")
                    text_placeholder = st.empty()

                    for new_text in streamer:
                        generated_text += new_text
                        # å®æ—¶æ›´æ–°æ˜¾ç¤º
                        text_placeholder.write(generated_text)
            else:
                # å¦‚æœæ²¡æœ‰å®¹å™¨ï¼Œåªæ”¶é›†æ–‡æœ¬
                for new_text in streamer:
                    generated_text += new_text

            thread.join()

            # ä¿å­˜åˆ°ç¼“å­˜
            if enable_cache and generated_text.strip():
                think_content, answer_content = parse_analysis_result(generated_text)
                # è¿™é‡Œéœ€è¦ä¼ å…¥å¤„ç†æ—¶é—´ï¼Œæš‚æ—¶è®¾ä¸º0ï¼Œåœ¨è°ƒç”¨å¤„ä¼šæ›´æ–°
                save_analysis_result(
                    image,
                    image_name,
                    model_type,
                    model_name,
                    prompt,
                    generated_text,
                    think_content,
                    answer_content,
                    0,
                )

            return generated_text.strip()

    except Exception as e:
        if output_container:
            with output_container:
                st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        return f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"


# ä¸»æ ‡é¢˜
st.markdown(
    '<h1 class="main-header">ğŸ¥ åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ</h1>', unsafe_allow_html=True
)

# ä¾§è¾¹æ  - ç³»ç»Ÿä¿¡æ¯å’Œé…ç½®
with st.sidebar:
    st.header("ğŸ”§ ç³»ç»Ÿé…ç½®")

    # è®¾å¤‡ä¿¡æ¯
    st.subheader("ğŸ’» è®¾å¤‡ä¿¡æ¯")
    device_info = get_device_info()
    
    # æ˜¾ç¤ºæ ¸å¿ƒè®¾å¤‡ä¿¡æ¯è¡¨æ ¼
    st.table(device_info)

    st.divider()

    # æ¨¡å‹é…ç½®
    st.subheader("ğŸ¤– æ¨¡å‹é…ç½®")

    # æ¨¡å‹é€‰æ‹©
    selected_model = st.selectbox(
        "é€‰æ‹©AIæ¨¡å‹",
        options=list(AVAILABLE_MODELS.keys()),
        index=list(AVAILABLE_MODELS.keys()).index(st.session_state.current_model),
        format_func=lambda x: f"{x} - {AVAILABLE_MODELS[x]['description']}",
        help="é€‰æ‹©ç”¨äºå›¾åƒåˆ†æçš„AIæ¨¡å‹",
    )

    # æ˜¾ç¤ºé€‰ä¸­æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯
    model_info = AVAILABLE_MODELS[selected_model]
    st.info(f"**æ¨¡å‹ç±»å‹ï¼š** {model_info['type']}\n**è·¯å¾„ï¼š** {model_info['path']}")

    # æ£€æŸ¥MedGemmaæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if selected_model == "MedGemma-4B":
        medgemma_path = model_info["path"]
        if not os.path.exists(medgemma_path):
            st.warning(f"âš ï¸ MedGemmaæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{medgemma_path}")
            st.info("ğŸ’¡ è¯·ç¡®ä¿å·²ä¸‹è½½MedGemmaæ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„")

    # æ¨¡å‹åŠ è½½çŠ¶æ€
    if (
        not st.session_state.model_loaded
        or st.session_state.current_model != selected_model
    ):
        if st.button(f"ğŸš€ åŠ è½½ {selected_model} æ¨¡å‹", type="primary"):
            with st.spinner(f"æ­£åœ¨åŠ è½½ {selected_model} æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                model, processor, device, model_type = load_model(selected_model)
                if model is not None:
                    st.session_state.model = model
                    st.session_state.processor = processor
                    st.session_state.device = device
                    st.session_state.current_model = selected_model
                    st.session_state.model_type = model_type
                    st.session_state.model_loaded = True
                    st.success(f"âœ… {selected_model} æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                    st.rerun()
    else:
        st.success(f"âœ… {st.session_state.current_model} æ¨¡å‹å·²å°±ç»ª")
        if st.button("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹"):
            st.session_state.model_loaded = False
            st.session_state.model = None
            st.session_state.processor = None
            st.session_state.device = None
            st.session_state.model_type = "glm4v"
            st.rerun()

    st.divider()

# ä¸»ç•Œé¢
if not st.session_state.model_loaded:
    st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ åŠ è½½GLM-4Væ¨¡å‹")

else:
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ–¼ï¸ å•å¼ å›¾åƒåˆ†æ", "ğŸ“Š æ‰¹é‡å¤„ç†", "ğŸ“‹ å†å²ç»“æœ", "ğŸ—ƒï¸ ç¼“å­˜ç®¡ç†"]
    )

    with tab1:
        st.header("å•å¼ å›¾åƒåˆ†æ")

        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            "é€‰æ‹©åŒ»å­¦å›¾åƒæ–‡ä»¶",
            type=["jpg", "jpeg", "png", "bmp", "tiff", "webp"],
            help="æ”¯æŒå¸¸è§çš„å›¾åƒæ ¼å¼ï¼Œæ–‡ä»¶å¤§å°é™åˆ¶10MB",
        )

        if uploaded_file is not None:
            # éªŒè¯æ–‡ä»¶
            is_valid, message = validate_image(uploaded_file)

            if is_valid:
                st.success(f"âœ… {message}")

                # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
                col1, col2 = st.columns([1, 1])

                with col1:
                    st.subheader("ğŸ“· åŸå§‹å›¾åƒ")

                    # æ˜¾ç¤ºå›¾åƒ
                    image = Image.open(uploaded_file)
                    st.image(
                        image, caption=f"æ–‡ä»¶å: {uploaded_file.name}", width="stretch"
                    )

                    # å›¾åƒä¿¡æ¯
                    st.markdown(
                        f"""
                    <div class="image-info">
                    ğŸ“ <strong>å°ºå¯¸ï¼š</strong> {image.size[0]} Ã— {image.size[1]} åƒç´ <br>
                    ğŸ¨ <strong>æ¨¡å¼ï¼š</strong> {image.mode}<br>
                    ğŸ“ <strong>å¤§å°ï¼š</strong> {uploaded_file.size / 1024:.1f} KB<br>
                    ğŸ“ <strong>æ ¼å¼ï¼š</strong> {image.format}
                    </div>
                    """,
                        unsafe_allow_html=True,
                    )

                with col2:
                    st.subheader("ğŸ¤– AIè¯Šæ–­åˆ†æ")

                    if st.button("ğŸ” å¼€å§‹åˆ†æ", type="primary"):
                        # åˆ›å»ºæµå¼è¾“å‡ºå®¹å™¨
                        streaming_container = st.empty()
                        result_containers = st.container()

                        try:
                            # å¤„ç†å›¾åƒ
                            start_time = time.time()

                            # è°ƒç”¨æµå¼åˆ†æ
                            generated_text = process_image_analysis(
                                image,
                                st.session_state.model,
                                st.session_state.processor,
                                st.session_state.device,
                                st.session_state.model_type,
                                streaming_container,
                                uploaded_file.name,
                                st.session_state.current_model,
                                True,  # enable_cache
                            )

                            end_time = time.time()
                            processing_time = end_time - start_time

                            # æ›´æ–°ç¼“å­˜ä¸­çš„å¤„ç†æ—¶é—´
                            update_processing_time(
                                image,
                                st.session_state.model_type,
                                st.session_state.current_model,
                                DEFAULT_MEDICAL_PROMPT,
                                processing_time,
                            )

                            # æ¸…ç©ºæµå¼è¾“å‡ºå®¹å™¨
                            streaming_container.empty()

                            # è§£æç»“æœ
                            think_content, answer_content = parse_analysis_result(
                                generated_text
                            )

                            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
                            with result_containers:
                                st.markdown(
                                    f"**â±ï¸ å¤„ç†æ—¶é—´ï¼š** {processing_time:.2f} ç§’"
                                )

                                if answer_content:
                                    with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True):
                                        st.write(answer_content)

                                if think_content:
                                    with st.expander("ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False):
                                        st.write(think_content)

                            # ä¿å­˜åˆ°å†å²è®°å½•
                            result_data = {
                                "timestamp": datetime.now().strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                ),
                                "filename": uploaded_file.name,
                                "image_size": f"{image.size[0]}x{image.size[1]}",
                                "processing_time": f"{processing_time:.2f}s",
                                "result": generated_text,
                                "think_content": think_content,
                                "answer_content": answer_content,
                            }
                            st.session_state.analysis_results.append(result_data)

                            # æä¾›ä¸‹è½½é€‰é¡¹
                            download_content = "åŒ»å­¦å›¾åƒè¯Šæ–­æŠ¥å‘Š\n"
                            download_content += "=" * 50 + "\n"
                            download_content += f"æ–‡ä»¶åï¼š{uploaded_file.name}\n"
                            download_content += f"åˆ†ææ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                            download_content += f"å¤„ç†æ—¶é—´ï¼š{processing_time:.2f}ç§’\n"
                            download_content += (
                                f"å›¾åƒå°ºå¯¸ï¼š{image.size[0]}x{image.size[1]}åƒç´ \n\n"
                            )

                            if answer_content:
                                download_content += (
                                    f"è¯Šæ–­ç»“æœï¼š\n{'-' * 30}\n{answer_content}\n\n"
                                )

                            if think_content:
                                download_content += (
                                    f"åˆ†æè¿‡ç¨‹ï¼š\n{'-' * 30}\n{think_content}\n"
                                )

                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½è¯Šæ–­æŠ¥å‘Š",
                                data=download_content,
                                file_name=f"{os.path.splitext(uploaded_file.name)[0]}_è¯Šæ–­æŠ¥å‘Š.txt",
                                mime="text/plain",
                            )

                        except Exception as e:
                            streaming_container.empty()
                            st.error(f"âŒ åˆ†æå¤±è´¥ï¼š{str(e)}")
            else:
                st.error(f"âŒ {message}")

    with tab2:
        st.header("æ‰¹é‡å›¾åƒå¤„ç†")

        # æ‰¹é‡æ–‡ä»¶ä¸Šä¼ 
        uploaded_files = st.file_uploader(
            "é€‰æ‹©å¤šå¼ åŒ»å­¦å›¾åƒæ–‡ä»¶",
            type=["jpg", "jpeg", "png", "bmp", "tiff", "webp"],
            accept_multiple_files=True,
            help="å¯ä»¥åŒæ—¶é€‰æ‹©å¤šå¼ å›¾åƒè¿›è¡Œæ‰¹é‡åˆ†æ",
        )

        if uploaded_files:
            st.success(f"âœ… å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")

            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            with st.expander("ğŸ“ æŸ¥çœ‹é€‰æ‹©çš„æ–‡ä»¶", expanded=True):
                for i, file in enumerate(uploaded_files, 1):
                    st.write(f"{i}. {file.name} ({file.size / 1024:.1f} KB)")

            start_batch = st.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ", type="primary")

            if start_batch:
                # æ‰¹é‡å¤„ç†
                total_start_time = time.time()

                # åˆ›å»ºè¿›åº¦æ˜¾ç¤º
                overall_progress = st.progress(0)
                status_container = st.empty()
                results_container = st.container()

                batch_results = []

                for i, uploaded_file in enumerate(uploaded_files):
                    # æ›´æ–°æ•´ä½“è¿›åº¦
                    progress = (i) / len(uploaded_files)
                    overall_progress.progress(progress)
                    status_container.write(
                        f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(uploaded_files)} ä¸ªæ–‡ä»¶: {uploaded_file.name}"
                    )

                    # éªŒè¯æ–‡ä»¶
                    is_valid, message = validate_image(uploaded_file)

                    if is_valid:
                        try:
                            # åŠ è½½å›¾åƒ
                            image = Image.open(uploaded_file)

                            # å¤„ç†å›¾åƒ
                            start_time = time.time()
                            generated_text = process_image_analysis(
                                image,
                                st.session_state.model,
                                st.session_state.processor,
                                st.session_state.device,
                                st.session_state.model_type,
                                None,  # output_container
                                uploaded_file.name,
                                st.session_state.current_model,
                                True,  # enable_cache
                            )
                            end_time = time.time()
                            processing_time = end_time - start_time

                            # æ›´æ–°ç¼“å­˜ä¸­çš„å¤„ç†æ—¶é—´
                            update_processing_time(
                                image,
                                st.session_state.model_type,
                                st.session_state.current_model,
                                DEFAULT_MEDICAL_PROMPT,
                                processing_time,
                            )

                            # è§£æç»“æœ
                            think_content, answer_content = parse_analysis_result(
                                generated_text
                            )

                            # ä¿å­˜ç»“æœ
                            result_data = {
                                "filename": uploaded_file.name,
                                "timestamp": datetime.now().strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                ),
                                "image_size": f"{image.size[0]}x{image.size[1]}",
                                "processing_time": f"{processing_time:.2f}s",
                                "result": generated_text,
                                "think_content": think_content,
                                "answer_content": answer_content,
                                "status": "success",
                            }
                            batch_results.append(result_data)
                            st.session_state.analysis_results.append(result_data)

                            # æ˜¾ç¤ºå•ä¸ªç»“æœ
                            with results_container:
                                with st.expander(
                                    f"âœ… {uploaded_file.name} - åˆ†æå®Œæˆ ({processing_time:.2f}s)"
                                ):
                                    col_img, col_result = st.columns([1, 2])
                                    with col_img:
                                        st.image(
                                            image,
                                            caption=uploaded_file.name,
                                            width="stretch",
                                        )
                                    with col_result:
                                        if answer_content:
                                            with st.expander(
                                                "ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True
                                            ):
                                                st.write(answer_content)
                                        if think_content:
                                            with st.expander(
                                                "ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False
                                            ):
                                                st.write(think_content)

                        except Exception as e:
                            result_data = {
                                "filename": uploaded_file.name,
                                "timestamp": datetime.now().strftime(
                                    "%Y-%m-%d %H:%M:%S"
                                ),
                                "error": str(e),
                                "status": "error",
                            }
                            batch_results.append(result_data)

                            with results_container:
                                st.error(f"âŒ {uploaded_file.name} å¤„ç†å¤±è´¥: {str(e)}")
                    else:
                        result_data = {
                            "filename": uploaded_file.name,
                            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "error": message,
                            "status": "invalid",
                        }
                        batch_results.append(result_data)

                        with results_container:
                            st.warning(f"âš ï¸ {uploaded_file.name}: {message}")

                # å®Œæˆæ‰¹é‡å¤„ç†
                overall_progress.progress(1.0)
                total_end_time = time.time()
                total_time = total_end_time - total_start_time

                status_container.success(
                    f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time:.2f}ç§’ï¼Œå¹³å‡æ¯å¼ ï¼š{total_time / len(uploaded_files):.2f}ç§’"
                )

                # æä¾›æ‰¹é‡ä¸‹è½½
                if batch_results:
                    # åˆ›å»ºæ‰¹é‡æŠ¥å‘Š
                    batch_report = f"æ‰¹é‡åŒ»å­¦å›¾åƒåˆ†ææŠ¥å‘Š\nç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                    batch_report += f"æ€»æ–‡ä»¶æ•°ï¼š{len(uploaded_files)}\n"
                    batch_report += f"æ€»å¤„ç†æ—¶é—´ï¼š{total_time:.2f}ç§’\n"
                    batch_report += (
                        f"å¹³å‡å¤„ç†æ—¶é—´ï¼š{total_time / len(uploaded_files):.2f}ç§’\n"
                    )
                    batch_report += "=" * 80 + "\n\n"

                    for result in batch_results:
                        batch_report += f"æ–‡ä»¶åï¼š{result['filename']}\n"
                        batch_report += f"æ—¶é—´æˆ³ï¼š{result['timestamp']}\n"
                        if result["status"] == "success":
                            batch_report += f"å›¾åƒå°ºå¯¸ï¼š{result['image_size']}\n"
                            batch_report += f"å¤„ç†æ—¶é—´ï¼š{result['processing_time']}\n"
                            batch_report += f"è¯Šæ–­ç»“æœï¼š\n{result['result']}\n"
                        else:
                            batch_report += "çŠ¶æ€ï¼šå¤„ç†å¤±è´¥\n"
                            batch_report += f"é”™è¯¯ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}\n"
                        batch_report += "\n" + "-" * 80 + "\n\n"

                    st.download_button(
                        label="ğŸ“¦ ä¸‹è½½æ‰¹é‡åˆ†ææŠ¥å‘Š",
                        data=batch_report,
                        file_name=f"æ‰¹é‡åˆ†ææŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain",
                    )

    with tab3:
        st.header("å†å²åˆ†æç»“æœ")

        if st.session_state.analysis_results:
            # ç»Ÿè®¡ä¿¡æ¯
            total_analyses = len(st.session_state.analysis_results)
            successful_analyses = sum(
                1
                for r in st.session_state.analysis_results
                if r.get("status") != "error"
            )

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»åˆ†ææ¬¡æ•°", total_analyses)
            with col2:
                st.metric("æˆåŠŸåˆ†æ", successful_analyses)
            with col3:
                st.metric(
                    "æˆåŠŸç‡", f"{successful_analyses / total_analyses * 100:.1f}%"
                )

            st.divider()

            # æ¸…ç©ºå†å²è®°å½•æŒ‰é’®
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå†å²è®°å½•"):
                st.session_state.analysis_results = []
                st.rerun()

            # æ˜¾ç¤ºå†å²ç»“æœ
            for i, result in enumerate(reversed(st.session_state.analysis_results)):
                with st.expander(
                    f"ğŸ“„ {result['filename']} - {result['timestamp']}", expanded=False
                ):
                    if result.get("status") == "success":
                        col_info, col_result = st.columns([1, 2])

                        with col_info:
                            st.write(f"**æ–‡ä»¶åï¼š** {result['filename']}")
                            st.write(f"**æ—¶é—´ï¼š** {result['timestamp']}")
                            if "image_size" in result:
                                st.write(f"**å°ºå¯¸ï¼š** {result['image_size']}")
                            if "processing_time" in result:
                                st.write(f"**å¤„ç†æ—¶é—´ï¼š** {result['processing_time']}")

                        with col_result:
                            # æ˜¾ç¤ºåˆ†ç¦»çš„ç»“æœ
                            if result.get("answer_content"):
                                with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True):
                                    st.write(result["answer_content"])

                            if result.get("think_content"):
                                with st.expander("ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False):
                                    st.write(result["think_content"])

                            # å¦‚æœæ²¡æœ‰åˆ†ç¦»çš„å†…å®¹ï¼Œæ˜¾ç¤ºåŸå§‹ç»“æœ
                            if not result.get("answer_content") and not result.get(
                                "think_content"
                            ):
                                st.write("**è¯Šæ–­ç»“æœï¼š**")
                                st.write(result["result"])

                            # å•ä¸ªç»“æœä¸‹è½½
                            download_data = "åŒ»å­¦å›¾åƒè¯Šæ–­æŠ¥å‘Š\n"
                            download_data += "=" * 50 + "\n"
                            download_data += f"æ–‡ä»¶åï¼š{result['filename']}\n"
                            download_data += f"åˆ†ææ—¶é—´ï¼š{result['timestamp']}\n"
                            if "processing_time" in result:
                                download_data += (
                                    f"å¤„ç†æ—¶é—´ï¼š{result['processing_time']}\n"
                                )
                            download_data += "\n"

                            if result.get("answer_content"):
                                download_data += f"è¯Šæ–­ç»“æœï¼š\n{'-' * 30}\n{result['answer_content']}\n\n"

                            if result.get("think_content"):
                                download_data += f"åˆ†æè¿‡ç¨‹ï¼š\n{'-' * 30}\n{result['think_content']}\n"

                            if not result.get("answer_content") and not result.get(
                                "think_content"
                            ):
                                download_data += f"è¯Šæ–­ç»“æœï¼š\n{result['result']}\n"

                            st.download_button(
                                label="ğŸ“¥ ä¸‹è½½æ­¤æŠ¥å‘Š",
                                data=download_data,
                                file_name=f"{os.path.splitext(result['filename'])[0]}_è¯Šæ–­æŠ¥å‘Š.txt",
                                mime="text/plain",
                                key=f"download_{i}",
                            )
                    else:
                        st.error(f"å¤„ç†å¤±è´¥ï¼š{result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            st.info("ğŸ“­ æš‚æ— å†å²åˆ†æç»“æœ")

    with tab4:
        st.header("ç¼“å­˜ç®¡ç†")

        st.markdown("""
        ### ğŸ—ƒï¸ æ•°æ®åº“ç¼“å­˜ç³»ç»Ÿ
        
        ç³»ç»Ÿä½¿ç”¨SQLiteæ•°æ®åº“ç¼“å­˜åˆ†æç»“æœï¼Œç›¸åŒå›¾ç‰‡å’Œæç¤ºè¯ç»„åˆå°†ç›´æ¥è¿”å›å·²æœ‰ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ã€‚
        
        **ç¼“å­˜æœºåˆ¶ï¼š**
        - ğŸ” **å›¾ç‰‡è¯†åˆ«**ï¼šåŸºäºå›¾ç‰‡å†…å®¹çš„MD5å“ˆå¸Œå€¼
        - ğŸ“ **æç¤ºè¯åŒ¹é…**ï¼šåŸºäºæç¤ºè¯å†…å®¹çš„MD5å“ˆå¸Œå€¼  
        - ğŸ¤– **æ¨¡å‹åŒºåˆ†**ï¼šä¸åŒæ¨¡å‹çš„ç»“æœåˆ†åˆ«ç¼“å­˜
        - âš¡ **å¿«é€ŸæŸ¥è¯¢**ï¼šæ•°æ®åº“ç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
        """)

        # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        try:
            conn = sqlite3.connect(DATABASE_PATH)
            cursor = conn.cursor()

            # æ€»è®°å½•æ•°
            cursor.execute("SELECT COUNT(*) FROM analysis_results")
            total_records = cursor.fetchone()[0]

            # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
            cursor.execute("""
                SELECT model_name, COUNT(*) 
                FROM analysis_results 
                GROUP BY model_name
            """)
            model_stats = cursor.fetchall()

            # æœ€è¿‘7å¤©çš„è®°å½•æ•°
            cursor.execute("""
                SELECT COUNT(*) FROM analysis_results 
                WHERE created_at >= datetime('now', '-7 days')
            """)
            recent_records = cursor.fetchone()[0]

            # æ•°æ®åº“å¤§å°
            cursor.execute(
                "SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()"
            )
            db_size = cursor.fetchone()[0] / 1024 / 1024  # MB

            conn.close()

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æ€»ç¼“å­˜è®°å½•", total_records)
            with col2:
                st.metric("è¿‘7å¤©è®°å½•", recent_records)
            with col3:
                st.metric("æ•°æ®åº“å¤§å°", f"{db_size:.2f} MB")
            with col4:
                st.metric("ç¼“å­˜æ–‡ä»¶", DATABASE_PATH)

            # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
            if model_stats:
                st.subheader("ğŸ“Š æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡")
                for model_name, count in model_stats:
                    st.write(f"**{model_name}**: {count} æ¬¡åˆ†æ")

            st.divider()

            # ç¼“å­˜å†å²è®°å½•
            st.subheader("ğŸ“‹ ç¼“å­˜å†å²è®°å½•")

            # è·å–è®°å½•æ•°é‡é€‰æ‹©
            record_limit = st.selectbox("æ˜¾ç¤ºè®°å½•æ•°", [10, 25, 50, 100], index=1)

            history_records = get_analysis_history(record_limit)

            if history_records:
                for i, record in enumerate(history_records):
                    with st.expander(
                        f"ğŸ“„ {record['image_name']} - {record['created_at'][:19]}",
                        expanded=False,
                    ):
                        col_info, col_result = st.columns([1, 2])

                        with col_info:
                            st.write(f"**æ–‡ä»¶åï¼š** {record['image_name']}")
                            st.write(f"**å›¾ç‰‡å°ºå¯¸ï¼š** {record['image_size']}")
                            st.write(
                                f"**ä½¿ç”¨æ¨¡å‹ï¼š** {record['model_name']} ({record['model_type']})"
                            )
                            st.write(
                                f"**å¤„ç†æ—¶é—´ï¼š** {record['processing_time']:.2f}ç§’"
                            )
                            st.write(f"**ç¼“å­˜æ—¶é—´ï¼š** {record['created_at'][:19]}")

                        with col_result:
                            # æ˜¾ç¤ºåˆ†ç¦»çš„ç»“æœ
                            if record.get("answer_content"):
                                with st.expander("ğŸ“‹ è¯Šæ–­ç»“æœ", expanded=True):
                                    st.write(record["answer_content"])

                            if record.get("think_content"):
                                with st.expander("ğŸ¤” åˆ†æè¿‡ç¨‹", expanded=False):
                                    st.write(record["think_content"])
            else:
                st.info("ğŸ“­ æš‚æ— ç¼“å­˜è®°å½•")

            st.divider()

            # ç¼“å­˜ç®¡ç†æ“ä½œ
            st.subheader("ğŸ› ï¸ ç¼“å­˜ç®¡ç†æ“ä½œ")

            col1, col2 = st.columns(2)

            with col1:
                if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç¼“å­˜", type="secondary"):
                    if clear_analysis_cache():
                        st.success("âœ… ç¼“å­˜å·²æ¸…ç©º")
                        st.rerun()
                    else:
                        st.error("âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥")

            with col2:
                # å¯¼å‡ºç¼“å­˜æ•°æ®
                if st.button("ğŸ“¤ å¯¼å‡ºç¼“å­˜æ•°æ®"):
                    try:
                        import pandas as pd

                        conn = sqlite3.connect(DATABASE_PATH)
                        df = pd.read_sql_query(
                            "SELECT * FROM analysis_results ORDER BY created_at DESC",
                            conn,
                        )
                        conn.close()

                        csv_data = df.to_csv(index=False)
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½CSVæ–‡ä»¶",
                            data=csv_data,
                            file_name=f"analysis_cache_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv",
                        )
                        st.success("âœ… å¯¼å‡ºæ•°æ®å‡†å¤‡å®Œæˆ")
                    except ImportError:
                        st.warning("âš ï¸ éœ€è¦å®‰è£…pandasåº“æ‰èƒ½å¯¼å‡ºCSV")
                    except Exception as e:
                        st.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

        except Exception as e:
            st.error(f"âŒ è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥ï¼š{e}")


# æ›´æ–°todoçŠ¶æ€
if st.session_state.model_loaded:
    # æ ‡è®°è®¾è®¡å’Œåˆ›å»ºä»»åŠ¡å®Œæˆ
    pass

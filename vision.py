from modelscope import AutoProcessor, Glm4vForConditionalGeneration
import torch
import time
import os
from PIL import Image
import sys
from transformers import TextIteratorStreamer

# è®¾ç½®ä½¿ç”¨GPU (RTX 5090)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(
        f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB"
    )


# è®¾ç½®å›¾ç‰‡ç›®å½•è·¯å¾„
SAMPLE_DIR = "sample"

# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
SUPPORTED_FORMATS = {".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"}


def get_image_files(directory):
    """è·å–ç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶"""
    if not os.path.exists(directory):
        print(f"âŒ é”™è¯¯ï¼šç›®å½• '{directory}' ä¸å­˜åœ¨ï¼")
        return []

    image_files = []
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        if os.path.isfile(file_path):
            file_ext = os.path.splitext(filename)[1].lower()
            if file_ext in SUPPORTED_FORMATS:
                image_files.append(file_path)

    return sorted(image_files)  # æŒ‰æ–‡ä»¶åæ’åº


def load_local_image(image_path):
    """åŠ è½½æœ¬åœ°å›¾ç‰‡æ–‡ä»¶"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        print(f"âŒ é”™è¯¯ï¼šå›¾ç‰‡æ–‡ä»¶ '{image_path}' ä¸å­˜åœ¨ï¼")
        return None

    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_ext = os.path.splitext(image_path)[1].lower()
    if file_ext not in SUPPORTED_FORMATS:
        print(f"âŒ é”™è¯¯ï¼šä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼ '{file_ext}'")
        print(f"ğŸ’¡ æ”¯æŒçš„æ ¼å¼ï¼š{', '.join(SUPPORTED_FORMATS)}")
        return None

    # åŠ è½½å›¾ç‰‡
    try:
        image = Image.open(image_path)
        print(f"âœ… æˆåŠŸåŠ è½½å›¾ç‰‡ï¼š{os.path.basename(image_path)}")
        print(f"ğŸ“ å›¾ç‰‡å°ºå¯¸ï¼š{image.size[0]} x {image.size[1]} åƒç´ ")
        print(f"ğŸ¨ å›¾ç‰‡æ¨¡å¼ï¼š{image.mode}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°ï¼š{os.path.getsize(image_path) / 1024:.1f} KB")
        return image
    except Exception as e:
        print(f"âŒ åŠ è½½å›¾ç‰‡å¤±è´¥ï¼š{e}")
        return None


# è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
image_files = get_image_files(SAMPLE_DIR)
if not image_files:
    print(f"âŒ åœ¨ç›®å½• '{SAMPLE_DIR}' ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶ï¼")
    print(f"ğŸ’¡ æ”¯æŒçš„æ ¼å¼ï¼š{', '.join(SUPPORTED_FORMATS)}")
    exit(1)

print(f"ğŸ” å‘ç° {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶ï¼š")
for i, file_path in enumerate(image_files, 1):
    print(f"  {i}. {os.path.basename(file_path)}")
print()

MODEL_PATH = "ZhipuAI/GLM-4.1V-9B-Thinking"

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
print("ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = Glm4vForConditionalGeneration.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="cuda:0",  # æ˜¾å¼æŒ‡å®šä½¿ç”¨ç¬¬ä¸€ä¸ªGPU (RTX 5090)
)
processor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")


def stream_generate(model, processor, inputs, max_new_tokens=8192):
    """æµå¼ç”Ÿæˆæ–‡æœ¬"""
    print("\n" + "=" * 60)
    print("ğŸ¤– AI å›¾ç‰‡æè¿°ç»“æœï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š")
    print("=" * 60)

    # è·å–è¾“å…¥é•¿åº¦
    input_length = inputs["input_ids"].shape[1]

    # ä½¿ç”¨æ¨¡å‹çš„generateæ–¹æ³•è¿›è¡Œæµå¼ç”Ÿæˆ
    streamer = TextIteratorStreamer(
        processor.tokenizer, skip_prompt=True, skip_special_tokens=False
    )

    # ç”Ÿæˆå‚æ•°
    generation_kwargs = dict(
        inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.8,
        do_sample=True,
        top_p=0.9,
        streamer=streamer,
    )

    # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
    import threading

    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # å®æ—¶è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
    generated_text = ""
    for new_text in streamer:
        generated_text += new_text
        print(new_text, end="", flush=True)

    thread.join()

    print("\n" + "=" * 60)
    return generated_text


def process_single_image(image_path, model, processor, device):
    """å¤„ç†å•ä¸ªå›¾ç‰‡"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ–¼ï¸  æ­£åœ¨å¤„ç†ï¼š{os.path.basename(image_path)}")
    print(f"{'=' * 80}")

    # åŠ è½½å›¾ç‰‡
    image = load_local_image(image_path)
    if image is None:
        print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ï¼š{os.path.basename(image_path)}")
        return

    # å‡†å¤‡æ¶ˆæ¯
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,  # ä½¿ç”¨PIL Imageå¯¹è±¡
                },
                {
                    "type": "text",
                    "text": "ä½ æ˜¯åŒ»å­¦é˜…ç‰‡ä¸“å®¶ï¼Œä»”ç»†æŸ¥çœ‹æ£€æŸ¥å›¾ç‰‡ï¼Œç»™å‡ºè¯¦ç»†çš„æ£€æŸ¥è¯Šæ–­ç»“æœ",
                },
            ],
        }
    ]

    # å‡†å¤‡è¾“å…¥
    time_start = time.time()
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt",
    ).to(device)  # ç¡®ä¿è¾“å…¥æ•°æ®ä¹Ÿåœ¨GPUä¸Š

    # æµå¼ç”Ÿæˆ
    print("ğŸš€ å¼€å§‹ç”Ÿæˆ...")
    generated_text = stream_generate(model, processor, inputs, max_new_tokens=8192)

    time_end = time.time()
    print(f"â±ï¸  å¤„ç†æ—¶é—´ï¼š{time_end - time_start:.2f} ç§’")

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    output_file = os.path.splitext(image_path)[0] + "_description.txt"
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"å›¾ç‰‡ï¼š{os.path.basename(image_path)}\n")
            f.write(f"å¤„ç†æ—¶é—´ï¼š{time_end - time_start:.2f} ç§’\n")
            f.write(f"æè¿°ï¼š\n{generated_text.strip()}\n")
        print(f"ğŸ’¾ æè¿°å·²ä¿å­˜åˆ°ï¼š{os.path.basename(output_file)}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{e}")


# æ‰¹é‡å¤„ç†æ‰€æœ‰å›¾ç‰‡
total_start_time = time.time()
print(f"ğŸ¯ å¼€å§‹æ‰¹é‡å¤„ç† {len(image_files)} ä¸ªå›¾ç‰‡æ–‡ä»¶...")

for i, image_path in enumerate(image_files, 1):
    print(f"\n{'ğŸ”„' * 20} è¿›åº¦ï¼š{i}/{len(image_files)} {'ğŸ”„' * 20}")
    process_single_image(image_path, model, processor, device)

total_end_time = time.time()
print(f"\n{'=' * 80}")
print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(image_files)} ä¸ªå›¾ç‰‡")
print(f"â±ï¸  æ€»è€—æ—¶ï¼š{total_end_time - total_start_time:.2f} ç§’")
print(
    f"âš¡ å¹³å‡æ¯å¼ å›¾ç‰‡ï¼š{(total_end_time - total_start_time) / len(image_files):.2f} ç§’"
)
print(f"{'=' * 80}")

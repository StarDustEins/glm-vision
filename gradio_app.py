from __future__ import annotations

import os
from types import SimpleNamespace
import json
from typing import Any, Dict, List, Optional, Tuple

import gradio as gr
import pandas as pd
from PIL import Image, ImageDraw

from app_logic import (
    DEFAULT_MEDICAL_PROMPT,
    AnalysisResult,
    build_available_models,
    fetch_cache_metrics,
    get_analysis_history,
    get_device_info,
    init_database,
    load_model,
    process_image_analysis,
    validate_image,
)


def _init_state() -> Dict[str, Any]:
    return {
        "model": None,
        "processor": None,
        "device": None,
        "model_type": None,
        "name": None,
    }


def _format_device_info() -> str:
    info = get_device_info()
    if not info:
        return "### ç¡¬ä»¶ä¿¡æ¯\n- æœªæ£€æµ‹åˆ°è®¾å¤‡ä¿¡æ¯"

    lines = ["### ç¡¬ä»¶ä¿¡æ¯"]
    for item in info:
        label = item.get("label", "è®¾å¤‡")
        value = item.get("value", "æœªçŸ¥")
        help_text = item.get("help")
        if help_text:
            lines.append(f"- **{label}**ï¼š{value}\\n  - {help_text}")
        else:
            lines.append(f"- **{label}**ï¼š{value}")
    return "\n".join(lines)


def _format_cache_metrics() -> str:
    metrics = fetch_cache_metrics()
    total = metrics.get("total_records", 0)
    recent = metrics.get("recent_records", 0)
    db_size = metrics.get("db_size_mb", 0.0)
    lines = ["### ç¼“å­˜æ¦‚è§ˆ"]
    lines.append(f"- æ€»ç¼“å­˜æ¡ç›®ï¼š{total}")
    lines.append(f"- æœ€è¿‘7å¤©åˆ†æï¼š{recent}")
    lines.append(f"- æ•°æ®åº“å¤§å°ï¼š{db_size:.2f} MB")
    return "\n".join(lines)


def _extract_structured_answer(
    answer_text: str,
) -> Tuple[str, List[Dict[str, float]]]:
    """å°†æ¨¡å‹è¿”å›çš„ JSON ç­”æ¡ˆè½¬æ¢ä¸º Markdownï¼ŒåŒæ—¶æå–æ ‡æ³¨æ¡†ã€‚"""
    cleaned = (answer_text or "").strip()
    if not cleaned:
        return "", []

    json_payload: Optional[Dict[str, Any]] = None
    if cleaned.startswith("{") and cleaned.endswith("}"):
        try:
            json_payload = json.loads(cleaned)
        except json.JSONDecodeError:
            json_payload = None
    else:
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = cleaned[start : end + 1]
            try:
                json_payload = json.loads(candidate)
            except json.JSONDecodeError:
                json_payload = None

    if not isinstance(json_payload, dict):
        return cleaned, []

    markers_field = json_payload.get("makers")
    if markers_field is None:
        markers_field = json_payload.get("markers")
    if markers_field is None:
        markers_field = []
    markers: List[Dict[str, float]] = []
    if isinstance(markers_field, list):
        for item in markers_field:
            if not isinstance(item, dict):
                continue
            try:
                height_value = item.get("height", item.get("hight"))
                marker = {
                    "x": float(item["x"]),
                    "y": float(item["y"]),
                    "width": float(item["width"]),
                    "height": float(height_value),
                }
            except (KeyError, TypeError, ValueError):
                continue
            markers.append(marker)

    section_map = {
        "éƒ¨ä½ä¿¡æ¯": json_payload.get("éƒ¨ä½ä¿¡æ¯", ""),
        "æ£€æŸ¥æ‰€è§": json_payload.get("æ£€æŸ¥æ‰€è§", ""),
        "è¯Šæ–­æ„è§": json_payload.get("è¯Šæ–­æ„è§", ""),
        "ç½®ä¿¡åº¦": json_payload.get("ç½®ä¿¡åº¦", ""),
    }

    lines = [
        f"1. **éƒ¨ä½ä¿¡æ¯**ï¼š{section_map['éƒ¨ä½ä¿¡æ¯']}",
        f"2. **æ£€æŸ¥æ‰€è§**ï¼š{section_map['æ£€æŸ¥æ‰€è§']}",
        f"3. **è¯Šæ–­æ„è§**ï¼š{section_map['è¯Šæ–­æ„è§']}",
        f"4. **ç½®ä¿¡åº¦**ï¼š{section_map['ç½®ä¿¡åº¦']}",
    ]

    if markers:
        lines.append(
            "5. **makers**ï¼š"
            + ", ".join(
                f"(x={m['x']:.3f}, y={m['y']:.3f}, w={m['width']:.3f}, h={m['height']:.3f})"
                for m in markers
            )
        )
    else:
        lines.append("5. **makers**ï¼š[]")

    return "\n".join(lines), markers


def _build_slider_images(
    base_image: Optional[Image.Image],
    markers: List[Dict[str, float]],
) -> Optional[Tuple[Image.Image, Image.Image]]:
    """æ„å»ºåŸå›¾ä¸æ ‡æ³¨å›¾å¯¹ï¼Œä¾› ImageSlider ä½¿ç”¨ã€‚"""
    if base_image is None:
        return None

    original = base_image.copy()
    annotated = base_image.copy()

    if not markers:
        return (original, annotated)

    draw = ImageDraw.Draw(annotated)
    width, height = annotated.size
    has_box = False

    for marker in markers:
        try:
            x = float(marker["x"])
            y = float(marker["y"])
            w = float(marker["width"])
            h = float(marker["height"])
        except (KeyError, TypeError, ValueError):
            continue

        x0 = max(0.0, min(1.0, x)) * width
        y0 = max(0.0, min(1.0, y)) * height
        x1 = max(0.0, min(1.0, x + w)) * width
        y1 = max(0.0, min(1.0, y + h)) * height

        if x1 <= x0 or y1 <= y0:
            continue

        draw.rectangle([x0, y0, x1, y1], outline="#ff4d4f", width=4)
        has_box = True

    if not has_box:
        return (original, original.copy())

    return (original, annotated)


def _build_history_dataframe(limit: int = 20) -> pd.DataFrame:
    records = get_analysis_history(limit=limit)
    if not records:
        return pd.DataFrame(
            columns=["ç”Ÿæˆæ—¶é—´", "æ¨¡å‹åç§°", "è€—æ—¶(s)", "å›¾åƒå°ºå¯¸", "è¯Šæ–­æ‘˜è¦"]
        )

    rows = []
    for record in records:
        summary = record.get("answer_content") or record.get("think_content") or ""
        summary = summary.replace("\n", " ")
        if len(summary) > 120:
            summary = summary[:117] + "..."
        rows.append(
            {
                "ç”Ÿæˆæ—¶é—´": record.get("created_at"),
                "æ¨¡å‹åç§°": record.get("model_name"),
                "è€—æ—¶(s)": round(float(record.get("processing_time", 0.0)), 2),
                "å›¾åƒå°ºå¯¸": record.get("image_size"),
                "è¯Šæ–­æ‘˜è¦": summary,
            }
        )

    return pd.DataFrame(rows)


def _compose_status(message: str, level: str = "info") -> str:
    level_class = {
        "info": "status-info",
        "warning": "status-warning",
        "error": "status-error",
        "success": "status-success",
    }.get(level, "status-info")
    return f"<div class='status-line {level_class}'>{message}</div>"


def _load_selected_model(
    model_name: str,
    state: Dict[str, Any],
    available_models: Dict[str, Dict[str, str]],
) -> Tuple[Dict[str, Any], str]:
    state = dict(state or {})
    if not model_name:
        return state, "âš ï¸ è¯·å…ˆé€‰æ‹©æ¨¡å‹å†åŠ è½½ã€‚"

    config = available_models.get(model_name)
    if config is None:
        return state, f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ï¼š{model_name}"

    try:
        model_bundle, processor, device, model_type = load_model(model_name, config)
    except Exception as exc:  # noqa: BLE001
        return state, f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{exc}"

    new_state = {
        "model": model_bundle,
        "processor": processor,
        "device": device,
        "model_type": model_type,
        "name": model_name,
    }

    device_repr = str(device)
    status = f"âœ… æ¨¡å‹å·²åŠ è½½ï¼š{model_name} ï¼ˆç±»å‹ï¼š{model_type}ï¼Œè®¾å¤‡ï¼š{device_repr}ï¼‰"
    return new_state, status


def _run_analysis(
    model_name: str,
    image_path: str,
    enable_cache: bool,
    state: Dict[str, Any],
    available_models: Dict[str, Dict[str, str]],
) -> Tuple[
    Dict[str, Any],
    str,
    str,
    str,
    pd.DataFrame,
    str,
    Optional[Tuple[Image.Image, Image.Image]],
]:
    state = dict(state or {})

    def _final(
        status_html: str,
        answer: str = "",
        think: str = "",
        slider_images: Optional[Tuple[Image.Image, Image.Image]] = None,
    ) -> Tuple[
        Dict[str, Any],
        str,
        str,
        str,
        pd.DataFrame,
        str,
        Optional[Tuple[Image.Image, Image.Image]],
    ]:
        return (
            state,
            status_html,
            answer,
            think,
            _build_history_dataframe(),
            _format_cache_metrics(),
            slider_images,
        )

    if not available_models:
        return _final(_compose_status("âŒ æœªå‘ç°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚", level="error"))

    if not model_name:
        return _final(
            _compose_status("âš ï¸ è¯·å…ˆé€‰æ‹©æ¨¡å‹ã€‚", level="warning"),
        )

    if model_name not in available_models:
        return _final(
            _compose_status(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹ï¼š{model_name}", level="error"),
        )

    if not image_path:
        return _final(
            _compose_status("âš ï¸ è¯·ä¸Šä¼ åŒ»å­¦å½±åƒæ–‡ä»¶ã€‚", level="warning"),
        )

    if state.get("name") != model_name or not state.get("model"):
        state, load_message = _load_selected_model(
            model_name,
            state,
            available_models,
        )
        state = dict(state or {})
        if not state.get("model"):
            return _final(
                _compose_status(str(load_message or "æ¨¡å‹åŠ è½½å¤±è´¥ã€‚"), level="error")
            )

    prompt_text = DEFAULT_MEDICAL_PROMPT.strip()

    display_image: Optional[Image.Image] = None
    processing_image: Optional[Image.Image] = None

    try:
        file_size = os.path.getsize(image_path)
        filename = os.path.basename(image_path)
        file_stub = SimpleNamespace(name=filename, size=file_size)
        is_valid, message = validate_image(file_stub)
        if not is_valid:
            return _final(
                _compose_status(f"âš ï¸ {message}", level="warning"),
            )
        with Image.open(image_path) as img:
            display_image = img.convert("RGB")
        processing_image = display_image.copy()
    except Exception as exc:  # noqa: BLE001
        return _final(
            _compose_status(f"âŒ å›¾åƒè¯»å–å¤±è´¥ï¼š{exc}", level="error"),
        )

    try:
        result = process_image_analysis(
            image=processing_image,
            model=state["model"],
            processor=state.get("processor"),
            device=state.get("device"),
            model_type=state.get("model_type", ""),
            model_name=state.get("name", ""),
            image_name=filename,
            prompt=prompt_text,
            enable_cache=enable_cache,
        )
    except Exception as exc:  # noqa: BLE001
        return _final(
            _compose_status(f"âŒ æ¨ç†è¿‡ç¨‹å‡ºç°é”™è¯¯ï¼š{exc}", level="error"),
        )

    status = _render_status(result)
    raw_answer = result.answer_content or result.raw_result
    formatted_answer, markers = _extract_structured_answer(raw_answer)
    slider_images = _build_slider_images(display_image, markers)
    think = result.think_content or "ï¼ˆæ¨¡å‹æœªè¿”å›æ˜¾æ€§æ¨ç†è¿‡ç¨‹ï¼‰"

    history_df = _build_history_dataframe()
    cache_stats = _format_cache_metrics()

    return state, status, formatted_answer, think, history_df, cache_stats, slider_images


def _render_status(result: AnalysisResult) -> str:
    flag = "ç¼“å­˜" if result.from_cache else "å®æ—¶æ¨ç†"
    created = result.created_at or "æœªçŸ¥æ—¶é—´"
    flag_class = "status-cache" if result.from_cache else "status-live"
    message = (
        "âœ… <span class='status-label'>å®Œæˆ</span> "
        f"<span class='{flag_class}'>{flag}</span>"
        f"<span class='status-body'>ï¼šè€—æ—¶ {result.processing_time:.2f}sï¼Œç”Ÿæˆæ—¶é—´ {created}</span>"
    )
    return _compose_status(message, level="success")


def _refresh_history_table() -> pd.DataFrame:
    return _build_history_dataframe()


def _refresh_cache_panel() -> str:
    return _format_cache_metrics()


def main() -> gr.Blocks:
    init_database()
    available_models = build_available_models()

    default_model = next(iter(available_models.keys()), None)
    history_df = _build_history_dataframe()
    cache_stats = _format_cache_metrics()

    theme = gr.themes.Soft()

    def _handle_analysis(
        selected_model: str,
        image_path: str,
        use_cache: bool,
        state: Dict[str, Any],
    ) -> Tuple[
        Dict[str, Any],
        str,
        str,
        str,
        pd.DataFrame,
        str,
        Optional[Tuple[Image.Image, Image.Image]],
    ]:
        (
            next_state,
            status_html,
            answer_md,
            think_md,
            history_df,
            cache_stats,
            slider_images,
        ) = _run_analysis(
            selected_model,
            image_path,
            use_cache,
            state,
            available_models,
        )
        slider_update = (
            gr.update(value=slider_images) if slider_images is not None else gr.update(value=None)
        )
        return (
            next_state,
            status_html,
            answer_md,
            think_md,
            history_df,
            cache_stats,
            slider_update,
        )

    def _on_analysis_start() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
        """æ¨ç†å¼€å§‹æ—¶ç¦ç”¨æŒ‰é’®å¹¶æç¤ºåŠ è½½çŠ¶æ€ã€‚"""
        return (
            gr.update(interactive=False, value="åˆ†æä¸­..."),
            gr.update(
                value=_compose_status("â³ æ­£åœ¨åˆ†æå›¾åƒï¼Œè¯·ç¨å€™...", level="info")
            ),
            gr.update(value=None),
        )

    def _on_analysis_end() -> Dict[str, Any]:
        """æ¨ç†ç»“æŸåæ¢å¤æŒ‰é’®å¯ç”¨çŠ¶æ€ã€‚"""
        return gr.update(interactive=True, value="å¼€å§‹åˆ†æ")

    with gr.Blocks(
        title="åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ",
        theme=theme,
    ) as demo:
        gr.Markdown("# åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ")
        gr.Markdown("ç»“åˆæœ¬åœ°æˆ–äº‘ç«¯æ¨¡å‹ï¼Œå¿«é€Ÿå®ŒæˆåŒ»å­¦å½±åƒçš„è‡ªåŠ¨è¯Šæ–­åˆ†æã€‚")

        model_state = gr.State(_init_state())

        with gr.Tabs(elem_id="page-tabs"):
            with gr.Tab("ä¸šåŠ¡"):
                with gr.Row(equal_height=True):
                    with gr.Column(scale=2, elem_id="inference-panel"):
                        image_slider = gr.ImageSlider(
                            label="åŸå§‹å›¾åƒ / æ ‡æ³¨å¯¹æ¯”",
                            value=None,
                            interactive=False,
                            height=420,
                            elem_id="preview-image",
                        )

                        status_display = gr.HTML(
                            _compose_status(
                                "ğŸ“Œ ç­‰å¾…åˆ†æï¼Œè¯·å…ˆä¸Šä¼ å›¾åƒã€‚", level="info"
                            ),
                            elem_id="status-box",
                        )

                    with gr.Column(scale=1, elem_id="control-panel"):
                        # gr.Markdown("### æ¨¡å‹ä¸å‚æ•°")
                        model_dropdown = gr.Dropdown(
                            choices=list(available_models.keys()),
                            value=default_model,
                            label="é€‰æ‹©æ¨¡å‹",
                            interactive=bool(available_models),
                        )

                        image_input = gr.File(
                            label="ä¸Šä¼ åŒ»å­¦å½±åƒæ–‡ä»¶",
                            file_types=["image"],
                            file_count="single",
                            type="filepath",
                        )

                        cache_checkbox = gr.Checkbox(label="å¯ç”¨ç¼“å­˜", value=True)
                        analyze_button = gr.Button("å¼€å§‹åˆ†æ", variant="primary")

                        if analyze_button is not None:
                            with gr.Tabs(elem_id="result-tabs"):
                                with gr.Tab("è¯Šæ–­ç»“æœ"):
                                    answer_md = gr.Markdown("å°šæœªç”Ÿæˆè¯Šæ–­å†…å®¹ã€‚")
                                with gr.Tab("æ¨ç†è¿‡ç¨‹"):
                                    think_md = gr.Markdown("æš‚æ— æ¨ç†å†…å®¹ã€‚")

            with gr.Tab("å†å²"):
                history_refresh = gr.Button("åˆ·æ–°å†å²", variant="secondary")
                history_table = gr.Dataframe(
                    value=history_df,
                    label="æœ€è¿‘åˆ†æè®°å½•",
                    interactive=False,
                )

            with gr.Tab("ç¼“å­˜"):
                cache_refresh = gr.Button("åˆ·æ–°ç¼“å­˜", variant="secondary")
                cache_md = gr.Markdown(cache_stats)

        analyze_button.click(
            fn=_on_analysis_start,
            inputs=None,
            outputs=[analyze_button, status_display, image_slider],
            queue=False,
        ).then(
            fn=_handle_analysis,
            inputs=[model_dropdown, image_input, cache_checkbox, model_state],
            outputs=[
                model_state,
                status_display,
                answer_md,
                think_md,
                history_table,
                cache_md,
                image_slider,
            ],
        ).then(
            fn=_on_analysis_end,
            inputs=None,
            outputs=analyze_button,
            queue=False,
        )

        history_refresh.click(
            fn=_refresh_history_table,
            inputs=None,
            outputs=history_table,
        )

        cache_refresh.click(
            fn=_refresh_cache_panel,
            inputs=None,
            outputs=cache_md,
        )

    return demo


if __name__ == "__main__":
    demo = main()
    demo.launch(
        pwa=True,
        debug=True,
        server_name="0.0.0.0",
        server_port=int(os.environ.get("PORT", 7860)),
    )

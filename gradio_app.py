from __future__ import annotations

import os
from types import SimpleNamespace
from typing import Any, Dict, Tuple

import gradio as gr
import pandas as pd
from PIL import Image

from app_logic import (
    DEFAULT_MEDICAL_PROMPT,
    AnalysisResult,
    build_available_models,
    fetch_cache_metrics,
    get_analysis_history,
    get_device_info,
    init_database,
    load_model,
    process_image_analysis,
    validate_image,
)


def _init_state() -> Dict[str, Any]:
    return {
        "model": None,
        "processor": None,
        "device": None,
        "model_type": None,
        "name": None,
    }


def _format_device_info() -> str:
    info = get_device_info()
    if not info:
        return "### ç¡¬ä»¶ä¿¡æ¯\n- æœªæ£€æµ‹åˆ°è®¾å¤‡ä¿¡æ¯"

    lines = ["### ç¡¬ä»¶ä¿¡æ¯"]
    for item in info:
        label = item.get("label", "è®¾å¤‡")
        value = item.get("value", "æœªçŸ¥")
        help_text = item.get("help")
        if help_text:
            lines.append(f"- **{label}**ï¼š{value}\\n  - {help_text}")
        else:
            lines.append(f"- **{label}**ï¼š{value}")
    return "\n".join(lines)


def _format_cache_metrics() -> str:
    metrics = fetch_cache_metrics()
    total = metrics.get("total_records", 0)
    recent = metrics.get("recent_records", 0)
    db_size = metrics.get("db_size_mb", 0.0)
    lines = ["### ç¼“å­˜æ¦‚è§ˆ"]
    lines.append(f"- æ€»ç¼“å­˜æ¡ç›®ï¼š{total}")
    lines.append(f"- æœ€è¿‘7å¤©åˆ†æï¼š{recent}")
    lines.append(f"- æ•°æ®åº“å¤§å°ï¼š{db_size:.2f} MB")
    return "\n".join(lines)


def _build_history_dataframe(limit: int = 20) -> pd.DataFrame:
    records = get_analysis_history(limit=limit)
    if not records:
        return pd.DataFrame(
            columns=["ç”Ÿæˆæ—¶é—´", "æ¨¡å‹åç§°", "è€—æ—¶(s)", "å›¾åƒå°ºå¯¸", "è¯Šæ–­æ‘˜è¦"]
        )

    rows = []
    for record in records:
        summary = record.get("answer_content") or record.get("think_content") or ""
        summary = summary.replace("\n", " ")
        if len(summary) > 120:
            summary = summary[:117] + "..."
        rows.append(
            {
                "ç”Ÿæˆæ—¶é—´": record.get("created_at"),
                "æ¨¡å‹åç§°": record.get("model_name"),
                "è€—æ—¶(s)": round(float(record.get("processing_time", 0.0)), 2),
                "å›¾åƒå°ºå¯¸": record.get("image_size"),
                "è¯Šæ–­æ‘˜è¦": summary,
            }
        )

    return pd.DataFrame(rows)


def _compose_status(message: str, level: str = "info") -> str:
    level_class = {
        "info": "status-info",
        "warning": "status-warning",
        "error": "status-error",
        "success": "status-success",
    }.get(level, "status-info")
    return f"<div class='status-line {level_class}'>{message}</div>"


def _load_selected_model(
    model_name: str,
    state: Dict[str, Any],
    available_models: Dict[str, Dict[str, str]],
) -> Tuple[Dict[str, Any], str]:
    state = dict(state or {})
    if not model_name:
        return state, "âš ï¸ è¯·å…ˆé€‰æ‹©æ¨¡å‹å†åŠ è½½ã€‚"

    config = available_models.get(model_name)
    if config is None:
        return state, f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ï¼š{model_name}"

    try:
        model_bundle, processor, device, model_type = load_model(model_name, config)
    except Exception as exc:  # noqa: BLE001
        return state, f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{exc}"

    new_state = {
        "model": model_bundle,
        "processor": processor,
        "device": device,
        "model_type": model_type,
        "name": model_name,
    }

    device_repr = str(device)
    status = f"âœ… æ¨¡å‹å·²åŠ è½½ï¼š{model_name} ï¼ˆç±»å‹ï¼š{model_type}ï¼Œè®¾å¤‡ï¼š{device_repr}ï¼‰"
    return new_state, status


def _run_analysis(
    model_name: str,
    image_path: str,
    enable_cache: bool,
    state: Dict[str, Any],
    available_models: Dict[str, Dict[str, str]],
) -> Tuple[Dict[str, Any], str, str, str, pd.DataFrame, str]:
    state = dict(state or {})

    def _final(
        status_html: str, answer: str = "", think: str = ""
    ) -> Tuple[
        Dict[str, Any],
        str,
        str,
        str,
        pd.DataFrame,
        str,
    ]:
        return (
            state,
            status_html,
            answer,
            think,
            _build_history_dataframe(),
            _format_cache_metrics(),
        )

    if not available_models:
        return _final(_compose_status("âŒ æœªå‘ç°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚", level="error"))

    if not model_name:
        return _final(
            _compose_status("âš ï¸ è¯·å…ˆé€‰æ‹©æ¨¡å‹ã€‚", level="warning"),
        )

    if model_name not in available_models:
        return _final(
            _compose_status(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹ï¼š{model_name}", level="error"),
        )

    if not image_path:
        return _final(
            _compose_status("âš ï¸ è¯·ä¸Šä¼ åŒ»å­¦å½±åƒæ–‡ä»¶ã€‚", level="warning"),
        )

    if state.get("name") != model_name or not state.get("model"):
        state, load_message = _load_selected_model(
            model_name,
            state,
            available_models,
        )
        state = dict(state or {})
        if not state.get("model"):
            return _final(
                _compose_status(str(load_message or "æ¨¡å‹åŠ è½½å¤±è´¥ã€‚"), level="error")
            )

    prompt_text = DEFAULT_MEDICAL_PROMPT.strip()

    try:
        file_size = os.path.getsize(image_path)
        filename = os.path.basename(image_path)
        file_stub = SimpleNamespace(name=filename, size=file_size)
        is_valid, message = validate_image(file_stub)
        if not is_valid:
            return _final(
                _compose_status(f"âš ï¸ {message}", level="warning"),
            )
        with Image.open(image_path) as img:
            image = img.convert("RGB")
    except Exception as exc:  # noqa: BLE001
        return _final(
            _compose_status(f"âŒ å›¾åƒè¯»å–å¤±è´¥ï¼š{exc}", level="error"),
        )

    try:
        result = process_image_analysis(
            image=image,
            model=state["model"],
            processor=state.get("processor"),
            device=state.get("device"),
            model_type=state.get("model_type", ""),
            model_name=state.get("name", ""),
            image_name=filename,
            prompt=prompt_text,
            enable_cache=enable_cache,
        )
    except Exception as exc:  # noqa: BLE001
        return _final(
            _compose_status(f"âŒ æ¨ç†è¿‡ç¨‹å‡ºç°é”™è¯¯ï¼š{exc}", level="error"),
        )

    status = _render_status(result)
    answer = result.answer_content or result.raw_result
    think = result.think_content or "ï¼ˆæ¨¡å‹æœªè¿”å›æ˜¾æ€§æ¨ç†è¿‡ç¨‹ï¼‰"

    history_df = _build_history_dataframe()
    cache_stats = _format_cache_metrics()

    return state, status, answer, think, history_df, cache_stats


def _render_status(result: AnalysisResult) -> str:
    flag = "ç¼“å­˜" if result.from_cache else "å®æ—¶æ¨ç†"
    created = result.created_at or "æœªçŸ¥æ—¶é—´"
    flag_class = "status-cache" if result.from_cache else "status-live"
    message = (
        "âœ… <span class='status-label'>å®Œæˆ</span> "
        f"<span class='{flag_class}'>{flag}</span>"
        f"<span class='status-body'>ï¼šè€—æ—¶ {result.processing_time:.2f}sï¼Œç”Ÿæˆæ—¶é—´ {created}</span>"
    )
    return _compose_status(message, level="success")


def _refresh_history_table() -> pd.DataFrame:
    return _build_history_dataframe()


def _refresh_cache_panel() -> str:
    return _format_cache_metrics()


def main() -> gr.Blocks:
    init_database()
    available_models = build_available_models()

    default_model = next(iter(available_models.keys()), None)
    history_df = _build_history_dataframe()
    cache_stats = _format_cache_metrics()

    theme = gr.themes.Soft()

    def _handle_analysis(
        selected_model: str,
        image_path: str,
        use_cache: bool,
        state: Dict[str, Any],
    ) -> Tuple[Dict[str, Any], str, str, str, pd.DataFrame, str]:
        return _run_analysis(
            selected_model,
            image_path,
            use_cache,
            state,
            available_models,
        )

    def _on_analysis_start() -> Tuple[gr.Button, gr.HTML]:
        """æ¨ç†å¼€å§‹æ—¶ç¦ç”¨æŒ‰é’®å¹¶æç¤ºåŠ è½½çŠ¶æ€ã€‚"""
        return (
            gr.Button.update(interactive=False),
            gr.update(
                value=_compose_status("â³ æ­£åœ¨åˆ†æå›¾åƒï¼Œè¯·ç¨å€™...", level="info")
            ),
        )

    def _on_analysis_end() -> gr.Button:
        """æ¨ç†ç»“æŸåæ¢å¤æŒ‰é’®å¯ç”¨çŠ¶æ€ã€‚"""
        return gr.Button.update(interactive=True)

    with gr.Blocks(
        title="åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ",
        theme=theme,
    ) as demo:
        gr.Markdown("# åŒ»å­¦å›¾åƒAIè¯Šæ–­ç³»ç»Ÿ")
        gr.Markdown("ç»“åˆæœ¬åœ°æˆ–äº‘ç«¯æ¨¡å‹ï¼Œå¿«é€Ÿå®ŒæˆåŒ»å­¦å½±åƒçš„è‡ªåŠ¨è¯Šæ–­åˆ†æã€‚")

        model_state = gr.State(_init_state())

        with gr.Tabs(elem_id="page-tabs"):
            with gr.Tab("ä¸šåŠ¡"):
                with gr.Row(equal_height=True):
                    with gr.Column(scale=2, elem_id="inference-panel"):
                        # gr.Markdown("### ä¸Šä¼ ä¸ç»“æœ")
                        image_input = gr.Image(
                            label="åŒ»å­¦å½±åƒ",
                            type="filepath",
                            sources=["upload"],
                            image_mode="RGB",
                            height=420,
                            elem_id="preview-image",
                        )

                        status_display = gr.HTML(
                            _compose_status(
                                "ğŸ“Œ ç­‰å¾…åˆ†æï¼Œè¯·å…ˆä¸Šä¼ å›¾åƒã€‚", level="info"
                            ),
                            elem_id="status-box",
                        )

                    with gr.Column(scale=1, elem_id="control-panel"):
                        # gr.Markdown("### æ¨¡å‹ä¸å‚æ•°")
                        model_dropdown = gr.Dropdown(
                            choices=list(available_models.keys()),
                            value=default_model,
                            label="é€‰æ‹©æ¨¡å‹",
                            interactive=bool(available_models),
                        )


                        cache_checkbox = gr.Checkbox(label="å¯ç”¨ç¼“å­˜", value=True)
                        analyze_button = gr.Button("å¼€å§‹åˆ†æ", variant="primary")

                        if analyze_button is not None:
                            with gr.Tabs(elem_id="result-tabs"):
                                with gr.Tab("è¯Šæ–­ç»“æœ"):
                                    answer_md = gr.Markdown("å°šæœªç”Ÿæˆè¯Šæ–­å†…å®¹ã€‚")
                                with gr.Tab("æ¨ç†è¿‡ç¨‹"):
                                    think_md = gr.Markdown("æš‚æ— æ¨ç†å†…å®¹ã€‚")

            with gr.Tab("å†å²"):
                history_refresh = gr.Button("åˆ·æ–°å†å²", variant="secondary")
                history_table = gr.Dataframe(
                    value=history_df,
                    label="æœ€è¿‘åˆ†æè®°å½•",
                    interactive=False,
                )

            with gr.Tab("ç¼“å­˜"):
                cache_refresh = gr.Button("åˆ·æ–°ç¼“å­˜", variant="secondary")
                cache_md = gr.Markdown(cache_stats)

        analyze_button.click(
            fn=_on_analysis_start,
            inputs=None,
            outputs=[analyze_button, status_display],
            queue=False,
        ).then(
            fn=_handle_analysis,
            inputs=[model_dropdown, image_input, cache_checkbox, model_state],
            outputs=[
                model_state,
                status_display,
                answer_md,
                think_md,
                history_table,
                cache_md,
            ],
        ).then(
            fn=_on_analysis_end,
            inputs=None,
            outputs=analyze_button,
            queue=False,
        )

        history_refresh.click(
            fn=_refresh_history_table,
            inputs=None,
            outputs=history_table,
        )

        cache_refresh.click(
            fn=_refresh_cache_panel,
            inputs=None,
            outputs=cache_md,
        )

    return demo


if __name__ == "__main__":
    demo = main()
    demo.launch(
        pwa=True,
        debug=True,
        server_name="0.0.0.0",
        server_port=int(os.environ.get("PORT", 7860)),
    )
